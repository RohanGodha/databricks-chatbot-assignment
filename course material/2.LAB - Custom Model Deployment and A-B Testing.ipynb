{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38748035-deb1-4e6d-80f3-3ee4d357175e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c64d9894-cfd0-456f-bef8-6ec89616c107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB: Custom Model Deployment and A/B Testing\n",
    "\n",
    "In this lab, you will learn how to deploy and serve a custom model using Databricks Model Serving. You will understand the steps involved in preparing, deploying, and querying a model endpoint in Databricks. This lab will focus on the practical aspects of deploying custom models and querying them for real-time inference.\n",
    "\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "*In this lab, you will need to complete the following tasks:*\n",
    "\n",
    "1. **Task 1:** Get Model Version\n",
    "1. **Task 2:** Deploy Model with SDK\n",
    "1. **Task 3:** Configure A/B Testing Using the UI\n",
    "1. **Task 4:** Query the Endpoint\n",
    "1. **Task 5:** Inspect Inference Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59f19abe-4989-48ac-adf9-f718209dbd9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5150e3f7-6fc4-464a-8f98-0f4fc3f17013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **15.4.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "411dc442-e7e2-497a-ab18-bb7aab13f741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c175a3-7799-4446-bf57-f3ad51470575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qq databricks-sdk\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55ba944a-1f24-4d66-a67c-9e235501920c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the Lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "271909fa-da90-4790-b0be-a224c12756f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15aba28f-c3fa-4d8e-804f-0cb6414cfca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this lab, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2dc4425-694b-4f76-ad59-dc7120a70d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser11195156_1755057414@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser11195156_1755057414\nWorking Directory: /Volumes/dbacademy/ops/labuser11195156_1755057414@vocareum_com\nDataset Location:  NestedNamespace (arxiv='/Volumes/dbacademy_arxiv/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49970b71-3a11-489e-adc8-ed7a3c537e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Model Details\n",
    "The model is created with the **00-Model-Build** notebook. It's registered in Unity Catalog for governance purposes and ease of deployment to Model Serving.\n",
    "\n",
    "Model location: `genai_shared_catalog.ws_<xxxx>.rag_app`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfecd893-4268-4e54-9d30-5b0c9a271036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: genai_shared_catalog_04.ws_1810110593084938.rag_app\n"
     ]
    }
   ],
   "source": [
    "shared_schema_name = f\"ws_{spark.conf.get('spark.databricks.clusterUsageTags.clusterOwnerOrgId')}\"\n",
    "model_name = f\"genai_shared_catalog_04.{shared_schema_name}.rag_app\"\n",
    "print(f\"Model name: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c62c59f-1885-40f3-a9b1-ff896c129025",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Task 1: Get Model Version\n",
    "\n",
    "In this task, you will retrieve model details and version from the model registry. This will help you identify the latest version of your model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5a3005-4b80-465a-9364-c03133cf2b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest model version: 2\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "\n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "## Set the registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "## Initialize the MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "## Get the latest version number of the specified model\n",
    "model_version_infos = client.search_model_versions(\"name = '%s'\" % model_name)\n",
    "if model_version_infos:\n",
    "    latest_model_version = max([model_version_info.version for model_version_info in model_version_infos])\n",
    "else:\n",
    "    raise(BaseException(\"Error: Model not created, verify if 00-Build-Model script ran successfully!\"))\n",
    "\n",
    "## Print the latest model version\n",
    "print(f\"Latest model version: {latest_model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cc94032-1f7c-430d-b6cf-9ab91f8b996f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Deploy Model with SDK\n",
    "\n",
    "In this task, you will deploy the model using the SDK and enable the inference table. This involves defining environment variables, configuring the endpoint, and setting up the inference table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75bac1a6-a1a0-43a5-94b3-7dd79acf82e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.1: Set Up Secrets\n",
    "\n",
    "\n",
    "To secure access to the serving endpoint, set up secrets for the host (workspace URL) and a personal access token. This can be done using the Databricks CLI:\n",
    "\n",
    "\n",
    "```\n",
    "databricks secrets create-scope <scope-name>\n",
    "databricks secrets put-secret --json '{\n",
    "  \"scope\": \"<scope-name>\",\n",
    "  \"key\": \"<key-name>\",\n",
    "  \"string_value\": \"<value>\"\n",
    "}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3e86280-8a32-43a1-855b-8cbdc0f32e3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Important:** Please note the syntax setup for the authentication above. Rather than passing the secret variables directly, we follow syntax requirements **&lcub;&lcub;secrets/&lt;scope&gt;/&lt;key-name&gt;&rcub;&rcub;** so that the endpoint will look up the secrets in real-time rather than automatically configure and expose static values.\n",
    "\n",
    "**To print the secret values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fd2473-0fbc-4838-8f79-5c8e983565e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":  genai_training_labuser11195156_1755057414\nKey for Token: depl_demo_token\nKey for Host: depl_demo_host\n"
     ]
    }
   ],
   "source": [
    "## Print the value of scope, key for token and key for host\n",
    "print(\": \", DA.scope_name)\n",
    "print(\"Key for Token: depl_demo_token\")\n",
    "print(\"Key for Host: depl_demo_host\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00790889-afe9-4470-88ed-3e9dda1c0d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2.2: Configure and Deploy Endpoint\n",
    "\n",
    "Configure the endpoint and deploy the model using the SDK, ensuring proper setup of environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38011003-9523-4463-bf29-43efda6a3a33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the endpoint dbc-ef7c3468-ef98.cloud.databricks.com/ml/endpoints/labuser11195156_1755057414_endpoint, this will take a few minutes to package and deploy the endpoint...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "Your Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/labuser11195156_1755057414_endpoint\">Model Serving Endpoint"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "## Define endpoint configuration\n",
    "endpoint_config_dict = {\n",
    "    \"served_models\": [\n",
    "        {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": latest_model_version,\n",
    "            \"scale_to_zero_enabled\": True,\n",
    "            \"workload_size\": \"Small\",\n",
    "            \"environment_vars\": {\n",
    "                \"DATABRICKS_TOKEN\": \"{{{{secrets/{0}/depl_demo_token}}}}\".format(DA.scope_name),\n",
    "                \"DATABRICKS_HOST\": \"{{{{secrets/{0}/depl_demo_host}}}}\".format(DA.scope_name),\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "endpoint_config = EndpointCoreConfigInput.from_dict(endpoint_config_dict)\n",
    "\n",
    "## Initiate the workspace client\n",
    "w = WorkspaceClient()\n",
    "serving_endpoint_name = f\"{DA.unique_name('_')}_endpoint\"\n",
    "\n",
    "## Get endpoint if it exists\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n",
    ")\n",
    "\n",
    "## Get the Databricks host\n",
    "db_host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"browserHostName\").value()\n",
    "serving_endpoint_url = f\"{db_host}/ml/endpoints/{serving_endpoint_name}\"\n",
    "\n",
    "## If endpoint doesn't exist, create it\n",
    "if existing_endpoint is None:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_url}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)\n",
    "## If endpoint does exist, update it to serve the new version\n",
    "else:\n",
    "    print(f\"Updating the endpoint {serving_endpoint_url} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name)\n",
    "\n",
    "## Display the endpoint URL\n",
    "displayHTML(f'Your Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/{serving_endpoint_name}\">Model Serving Endpoint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256b2fbb-4a80-4435-933a-411bd06ee6f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3: Create Inference Table via Model Serving UI\n",
    "\n",
    "Set up an inference table through the Model Serving UI:\n",
    "\n",
    "1. Click the link printed above in the output of **step 2.2**.\n",
    "2. Click the **Edit** button.\n",
    "3. Expand the **Inference tables** section.\n",
    "4. Check the **Enable inference tables** box.\n",
    "5. Enter the catalog, schema, and table information for the inference table:\n",
    "   - **Catalog Name:** `<Your Catalog Name>`\n",
    "   - **Schema Name:** `<Your Schema Name>`\n",
    "   - **Table Name Prefix:** `<Your Table Name>` (e.g.: `rag_app_realtime`)\n",
    "6. Click **Update** button"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a9a2aee-efbc-4cbf-9590-8e6484c7d3b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Configure A/B Testing Using the UI\n",
    "\n",
    "In this task, you will configure traffic splitting between the same version of the model for A/B testing using the Databricks UI. This will ensure that both configurations are available for inference, and you can direct a percentage of traffic to each configuration for A/B testing or gradual rollouts.\n",
    "\n",
    "\n",
    "\uD83D\uDEA8 **Note:** Normally, you would register an improved version of the model. However, due to time constraints, you will deploy the same model that we served.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Go to [Serving](/ml/endpoints)**\n",
    "\n",
    "2. Locate the endpoint you created earlier.\n",
    "3. Click on the **Edit** button next to the endpoint name.\n",
    "\n",
    "4. **Add a New Served Entity**\n",
    "    - In the **Served entities** section, click on **+ Add served entity**.\n",
    "    - Under **My Models**, select the entity name that matches your model name: **`genai_shared_catalog.ws_<xxx>.rag_app`**. Model name is printed in the beginning of this notebook.\n",
    "    - Choose **Version 1** for the new served entity.\n",
    "\n",
    "5. **Configure Traffic Splitting**\n",
    "    - In the **Traffic Splitting** section, divide the traffic between the two configurations.\n",
    "    - Set the traffic percentage to 60% for the new configuration and 40% for the old configuration.\n",
    "\n",
    "6. **Set Compute Scale-out**\n",
    "    - For **Compute scale-out**, select **Small**.\n",
    "\n",
    "7. **Advanced Configuration**\n",
    "    - Fill in the environment variables as follows (**\uD83D\uDCA1These values are printed at the beginning of the lab. Replace the value of token_key and host_key**.):\n",
    "      - **DATABRICKS_HOST** : **&lcub;&lcub; secrets/`scope`/`host_key` &rcub;&rcub;**\n",
    "      - **DATABRICKS_TOKEN** : **&lcub;&lcub; secrets/`scope`/`token_key` &rcub;&rcub;**\n",
    "    - **\uD83D\uDCCCNOTE : Add a unique served entity name.**\n",
    "\n",
    "8. **Check Inference Table Details**\n",
    "    - Ensure that the inference table settings are correct.\n",
    "    - The table should capture inference results for analysis.\n",
    "9. **Rename the Served Entity**\n",
    "    - In the Advanced Configuration section, update the Served entity name.\n",
    "    - Change it from the default rag_app-1 to a unique name such as rag_app-2 for the **_new configuration_**.\n",
    "        > If not updated, you may encounter the following error:\n",
    "    `Served entities must have unique served entity names. Check the advanced configurations of your served entity.`\n",
    "10. **Update and Wait**\n",
    "    - Click on the **Update** button to save your changes.\n",
    "    - Wait for the serving endpoint to update. This may take a few minutes.\n",
    "\n",
    "By following these steps, you will successfully configure A/B testing for your model using the same version, allowing you to evaluate different configurations and monitor their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a9208d6-7f02-46d1-8c53-b6e50c079a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Query the Endpoint \n",
    "\n",
    "In this task, you will query the model using Databricks SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c188f4-c2a3-4a6d-b5af-e4e3021407c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PPO stands for Proximal Policy Optimization, a reinforcement learning algorithm used to train models, particularly in the context of Generative AI.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "## Define the question to be sent to the model for inference\n",
    "messages = {\"messages\" : [\n",
    "        {\"role\": \"user\", \"content\": \"What is PPO?\"}\n",
    "    ]}\n",
    "\n",
    "## Send the query to the specified serving endpoint and receive the response\n",
    "answer = w.serving_endpoints.query(\n",
    "  serving_endpoint_name, \n",
    "  inputs=messages\n",
    ")\n",
    "\n",
    "## Print the model's prediction from the response received\n",
    "print(answer.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41b637c4-6cf6-424a-9440-c094e851e957",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 5: Inspect Inference Table\n",
    "\n",
    "In this task, you will view and inspect the inference table created during the deployment process. The inference table stores data about the inferences made by your model, which can be useful for monitoring and analyzing model performance.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Go to [Catalog](explore/data).**\n",
    "\n",
    "2. **Select the Catalog and Schema:**\n",
    "   - In the Catalog Explorer, find and select the catalog that you entered while configuring the inference table.\n",
    "   - Within the selected catalog, navigate to the schema that contains your inference table.\n",
    "\n",
    "3. **View the Inference Table:**\n",
    "   - Locate the inference table within the selected schema. The table name is prefixed as specified during the deployment configuration.\n",
    "   - Click on the inference table to open and view the sample data stored in it.\n",
    "\n",
    "By following these steps, you will be able to access and inspect the inference data stored in the table, allowing you to analyze how your model is performing and what kind of predictions it is making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6eb1477-4498-4903-8d50-5bc35b567d7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you successfully deployed a custom model using Databricks Model Serving. You learned how to retrieve model versions, deploy models using the SDK, create and deploy a second version using the UI, query the model endpoint, and inspect inference results stored in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dc4f0c5-beb2-458d-ba46-4a6ef3e8256f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.LAB - Custom Model Deployment and A-B Testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}