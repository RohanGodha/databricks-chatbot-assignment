{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d2f8177-bb85-4532-9035-2dfe453f6dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b13126f6-e605-4cd2-8ed6-517e81b4bded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB- Evaluation with Mosaic AI Agent Evaluation\n",
    "\n",
    "In this lab, you will have the opportunity to evaluate a RAG chain model **using Mosaic AI Agent Evaluation Framework.**\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "*In this lab, you will complete the following tasks:*\n",
    "\n",
    "- **Task 1**: Define a custom Gen AI evaluation metric.\n",
    "\n",
    "- **Task 2**: Conduct an evaluation test using the Agent Evaluation Framework.\n",
    "\n",
    "- **Task 3**: Analyze the evaluation results through the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81a33a2e-0356-4150-8e35-e143ff578c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a28fa805-65b6-453c-a457-8968d871a44b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **15.4.x-cpu-ml-scala2.12**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc641d3-faff-4936-b13c-a14b3859e5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89723bb5-b775-424e-a3f7-29d9cf114475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\njupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.10.0 which is incompatible.\nlangchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.63 which is incompatible.\nlangchain-community 0.0.38 requires tenacity<9.0.0,>=8.1.0, but you have tenacity 9.1.2 which is incompatible.\nnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.11.7 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qq databricks-agents databricks-sdk databricks-vectorsearch langchain-databricks langchain==0.3.7\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "147bcfff-9085-4700-9513-846736195793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Before starting the lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9520321c-7b7f-403b-a293-49fefefc1053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b54dd49f-48f3-427a-83f1-db602486134c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this lab, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da31fb8a-f14e-49e7-8659-2ab9bddd91bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser11185624_1754988112@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser11185624_1754988112\nWorking Directory: /Volumes/dbacademy/ops/labuser11185624_1754988112@vocareum_com\nDataset Location:  NestedNamespace (news='/Volumes/dbacademy_news/v01', arxiv='/Volumes/dbacademy_arxiv/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca83d422-7134-4a83-b3e6-c38833eb4762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lab Overview\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a5ead1c-01ec-45b4-b60b-56bed9c0896d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluation Dataset\n",
    "\n",
    "In this lab, you will work with the same dataset utilized in the demos. This dataset contains sample queries along with their corresponding expected responses, which are generated using synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb76b9e1-c8dd-4c93-9a98-b9d0e81c3669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>request</th><th>expected_response</th><th>evolution_type</th><th>episode_done</th></tr></thead><tbody><tr><td>What are the limitations of symbolic planning in task and motion planning, and how can leveraging large language models help overcome these limitations?</td><td>Symbolic planning in task and motion planning can be limited by the need for explicit primitives and constraints. Leveraging large language models can help overcome these limitations by enabling the robot to use language models for planning and execution, and by providing a way to extract and leverage knowledge from large language models to solve temporally extended tasks.</td><td>simple</td><td>true</td></tr><tr><td>What are some techniques used to fine-tune transformer models for personalized code generation, and how effective are they in improving prediction accuracy and preventing runtime errors? </td><td>The techniques used to fine-tune transformer models for personalized code generation include ﬁne-tuning transformer models, adopting a novel approach called Target Similarity Tuning (TST) to retrieve a small set of examples from a training bank, and utilizing these examples to train a pretrained language model. The effectiveness of these techniques is shown in the improvement in prediction accuracy and the prevention of runtime errors.</td><td>simple</td><td>true</td></tr><tr><td>How does the PPO-ptx model mitigate performance regressions in the few-shot setting?</td><td>The PPO-ptx model mitigates performance regressions in the few-shot setting by incorporating pre-training and fine-tuning on the downstream task. This approach allows the model to learn generalizable features and adapt to new tasks more effectively, leading to improved few-shot performance.</td><td>simple</td><td>true</td></tr><tr><td>How can complex questions be decomposed using successive prompting?</td><td>Successive prompting is a method for decomposing complex questions into simpler sub-questions, allowing language models to answer them more accurately. This approach was proposed by Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner in their paper 'Successive Prompting for Decomposing Complex Questions', presented at EMNLP 2022.</td><td>simple</td><td>true</td></tr><tr><td>Which entity type in Named Entity Recognition is likely to be involved in information extraction, question answering, semantic parsing, and machine translation?</td><td>Organization</td><td>reasoning</td><td>true</td></tr><tr><td>What is the purpose of ROUGE (Recall-Oriented Understudy for Gisting Evaluation) in automatic evaluation methods?</td><td>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is used in automatic evaluation methods to evaluate the quality of machine translation. It calculates N-gram co-occurrence statistics, which are used to assess the similarity between the candidate text and the reference text. ROUGE is based on recall, whereas BLEU is based on accuracy.</td><td>simple</td><td>true</td></tr><tr><td>What are the challenges associated with Foundation SSL in CV, and how do they relate to the lack of theoretical foundation, semantic understanding, and explicable exploration?</td><td>The challenges associated with Foundation SSL in CV include the lack of a profound theory to support all kinds of tentative experiments, and further exploration has no handbook. The pretrained LM may not learn the meaning of the language, relying on corpus learning instead. The models cannot reach a better level of stability and match different downstream tasks, and the primary method is to increase data, improve computation power, and design training procedures to achieve better results. The lack of theoretical foundation, semantic understanding, and explicable exploration are the main challenges in Foundation SSL in CV.</td><td>simple</td><td>true</td></tr><tr><td>How does ChatGPT handle factual input compared to GPT-3.5?</td><td>ChatGPT handles factual input better than GPT-3.5, with a 21.9% increase in accuracy when the premise entails the hypothesis. This is possibly related to the preference for human feedback in ChatGPT's RLHF design during model training.</td><td>simple</td><td>true</td></tr><tr><td>What are some of the challenges in understanding natural language commands for robotic navigation and mobile manipulation?</td><td>Some challenges in understanding natural language commands for robotic navigation and mobile manipulation include integrating natural language understanding with reinforcement learning, understanding natural language directions for robotic navigation, and mapping instructions and visual observations to actions with reinforcement learning.</td><td>simple</td><td>true</td></tr><tr><td>How does chain of thought prompting elicit reasoning in large language models, and what are the potential applications of this technique in neural text generation and human-AI interaction?</td><td>The context discusses the use of chain of thought prompting to elicit reasoning in large language models, which can be applied in neural text generation and human-AI interaction. Specifically, researchers have used this technique to train language models to generate coherent and contextually relevant text, and to create transparent and controllable human-AI interaction systems. The potential applications of this technique include improving the performance of language models in generating contextually appropriate responses, enhancing the interpretability and controllability of AI systems, and facilitating more effective human-AI collaboration.</td><td>simple</td><td>true</td></tr><tr><td>Using the given context, how can the robot be instructed to move objects around on a tabletop to complete rearrangement tasks?</td><td>The robot can be instructed to move objects around on a tabletop to complete rearrangement tasks by using natural language instructions that specify the objects to be moved and their desired locations. The instructions can be parsed using functions such as parse_obj_name and parse_position to extract the necessary information, and then passed to a motion primitive that can pick up and place objects in the specified locations. The get_obj_names and get_obj_pos APIs can be used to access information about the available objects and their locations in the scene.</td><td>reasoning</td><td>true</td></tr><tr><td>How can searching over an organization's existing knowledge, data, or documents using LLM-powered applications reduce the time it takes to complete worker activities?</td><td>Using AI tools to search through a company's information can significantly cut down the time workers need to finish tasks. These tools quickly find and provide the necessary details by scanning large amounts of data efficiently.</td><td>simple</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "What are the limitations of symbolic planning in task and motion planning, and how can leveraging large language models help overcome these limitations?",
         "Symbolic planning in task and motion planning can be limited by the need for explicit primitives and constraints. Leveraging large language models can help overcome these limitations by enabling the robot to use language models for planning and execution, and by providing a way to extract and leverage knowledge from large language models to solve temporally extended tasks.",
         "simple",
         true
        ],
        [
         "What are some techniques used to fine-tune transformer models for personalized code generation, and how effective are they in improving prediction accuracy and preventing runtime errors? ",
         "The techniques used to fine-tune transformer models for personalized code generation include ﬁne-tuning transformer models, adopting a novel approach called Target Similarity Tuning (TST) to retrieve a small set of examples from a training bank, and utilizing these examples to train a pretrained language model. The effectiveness of these techniques is shown in the improvement in prediction accuracy and the prevention of runtime errors.",
         "simple",
         true
        ],
        [
         "How does the PPO-ptx model mitigate performance regressions in the few-shot setting?",
         "The PPO-ptx model mitigates performance regressions in the few-shot setting by incorporating pre-training and fine-tuning on the downstream task. This approach allows the model to learn generalizable features and adapt to new tasks more effectively, leading to improved few-shot performance.",
         "simple",
         true
        ],
        [
         "How can complex questions be decomposed using successive prompting?",
         "Successive prompting is a method for decomposing complex questions into simpler sub-questions, allowing language models to answer them more accurately. This approach was proposed by Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner in their paper 'Successive Prompting for Decomposing Complex Questions', presented at EMNLP 2022.",
         "simple",
         true
        ],
        [
         "Which entity type in Named Entity Recognition is likely to be involved in information extraction, question answering, semantic parsing, and machine translation?",
         "Organization",
         "reasoning",
         true
        ],
        [
         "What is the purpose of ROUGE (Recall-Oriented Understudy for Gisting Evaluation) in automatic evaluation methods?",
         "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is used in automatic evaluation methods to evaluate the quality of machine translation. It calculates N-gram co-occurrence statistics, which are used to assess the similarity between the candidate text and the reference text. ROUGE is based on recall, whereas BLEU is based on accuracy.",
         "simple",
         true
        ],
        [
         "What are the challenges associated with Foundation SSL in CV, and how do they relate to the lack of theoretical foundation, semantic understanding, and explicable exploration?",
         "The challenges associated with Foundation SSL in CV include the lack of a profound theory to support all kinds of tentative experiments, and further exploration has no handbook. The pretrained LM may not learn the meaning of the language, relying on corpus learning instead. The models cannot reach a better level of stability and match different downstream tasks, and the primary method is to increase data, improve computation power, and design training procedures to achieve better results. The lack of theoretical foundation, semantic understanding, and explicable exploration are the main challenges in Foundation SSL in CV.",
         "simple",
         true
        ],
        [
         "How does ChatGPT handle factual input compared to GPT-3.5?",
         "ChatGPT handles factual input better than GPT-3.5, with a 21.9% increase in accuracy when the premise entails the hypothesis. This is possibly related to the preference for human feedback in ChatGPT's RLHF design during model training.",
         "simple",
         true
        ],
        [
         "What are some of the challenges in understanding natural language commands for robotic navigation and mobile manipulation?",
         "Some challenges in understanding natural language commands for robotic navigation and mobile manipulation include integrating natural language understanding with reinforcement learning, understanding natural language directions for robotic navigation, and mapping instructions and visual observations to actions with reinforcement learning.",
         "simple",
         true
        ],
        [
         "How does chain of thought prompting elicit reasoning in large language models, and what are the potential applications of this technique in neural text generation and human-AI interaction?",
         "The context discusses the use of chain of thought prompting to elicit reasoning in large language models, which can be applied in neural text generation and human-AI interaction. Specifically, researchers have used this technique to train language models to generate coherent and contextually relevant text, and to create transparent and controllable human-AI interaction systems. The potential applications of this technique include improving the performance of language models in generating contextually appropriate responses, enhancing the interpretability and controllability of AI systems, and facilitating more effective human-AI collaboration.",
         "simple",
         true
        ],
        [
         "Using the given context, how can the robot be instructed to move objects around on a tabletop to complete rearrangement tasks?",
         "The robot can be instructed to move objects around on a tabletop to complete rearrangement tasks by using natural language instructions that specify the objects to be moved and their desired locations. The instructions can be parsed using functions such as parse_obj_name and parse_position to extract the necessary information, and then passed to a motion primitive that can pick up and place objects in the specified locations. The get_obj_names and get_obj_pos APIs can be used to access information about the available objects and their locations in the scene.",
         "reasoning",
         true
        ],
        [
         "How can searching over an organization's existing knowledge, data, or documents using LLM-powered applications reduce the time it takes to complete worker activities?",
         "Using AI tools to search through a company's information can significantly cut down the time workers need to finish tasks. These tools quickly find and provide the necessary details by scanning large amounts of data efficiently.",
         "simple",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "request",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "expected_response",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "evolution_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "episode_done",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(DA.eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b5949c5-d381-4ddd-a822-d3ae6e8b8166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Load the Model\n",
    "\n",
    "A RAG chain has been created and registered for use in this lab. The model details are provided below.\n",
    "\n",
    "**\uD83D\uDCCC Note:** If you are using your own workspace to run this lab, you must manually execute **`00 - Build Model / 00-Build Model`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d218e0b2-5429-4a97-a0ce-c1b1bfd10912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "catalog_name = \"genai_shared_catalog_03\"\n",
    "schema_name = f\"ws_{spark.conf.get('spark.databricks.clusterUsageTags.clusterOwnerOrgId')}\"\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "model_uri = f\"models:/{catalog_name}.{schema_name}.rag_app/1\"\n",
    "model_name = f\"{catalog_name}.{schema_name}.rag_app\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13a08a1-9cfc-46b1-88b6-15079e47145f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1 - Define A Custom Metric\n",
    "\n",
    "For this task, define a custom metric to evaluate whether the generated **\"ANSWER\"** from the RAG chain is easily readable by a non-expert user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564a64b9-4d62-4389-b76a-af85840e2b9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.metrics.genai import make_genai_metric_from_prompt\n",
    "\n",
    "## Prompt for LLM as judge to determine if the generated response is easily readable by non-academic or expert readers\n",
    "eval_prompt = \"Your task is to determine whether the generated response easily readable by non-academic or expert readers. This was the content: '{retrieved_context}'\"\n",
    "\n",
    "## Use Llama-3 as LLM\n",
    "llm=\"endpoints:/databricks-meta-llama-3-3-70b-instruct\"\n",
    "\n",
    "## Define the metric\n",
    "is_readable = make_genai_metric_from_prompt(\n",
    "    name=\"is_readable\",\n",
    "    judge_prompt=eval_prompt,\n",
    "    model=llm,\n",
    "    metric_metadata={\"assessment_type\": \"ANSWER\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1913f5-76fe-4f5c-82a1-fe3789e407af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 2 - Run Evaluation Test\n",
    "\n",
    "Next, run an evaluation using the custom metric you defined. Ensure that you select **Mosaic AI Agent Evaluation** as the evaluation type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4307f0e-6540-4143-a396-890f8c8a579f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b0fe843e8a403e933d2d6f73f96611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/12 09:30:33 WARNING mlflow.utils.requirements_utils: Detected one or more mismatches between the model's dependencies and the current Python environment:\n - langchain-community (current: 0.0.38, required: langchain-community==0.3.7)\nTo fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bbb9b10c-3750-484e-98ed-7f3eb432570b/lib/python3.11/site-packages/langchain/chains/api/base.py:56: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n\n  from langchain_community.utilities.requests import TextRequestsWrapper\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-bbb9b10c-3750-484e-98ed-7f3eb432570b/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `TextRequestsWrapper` to V2.\n  warn(\n2025/08/12 09:30:34 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b4c73b6c4349709ace3fef7f5a37e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [Elapsed: 00:00, Remaining: ?]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/1187/command-29457720978499-3220231051:25: LangChainDeprecationWarning: Use databricks_langchain.DatabricksVectorSearch\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://dbc-c1d36e8f-7273.cloud.databricks.com/ml/experiments/3841843043481060/runs/9e4b8c0c2a6143aa997ca2de4922678c/evaluations/\" class=\"button\">\n",
       "            View evaluation results: 0/12 pass\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with mlflow.start_run(run_name=\"lab_04_agent_evaluation\"):\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data = DA.eval_df,\n",
    "        model = model_uri,\n",
    "        model_type = \"databricks-agent\",\n",
    "        extra_metrics=[is_readable]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0adc0ec4-04df-43b1-8ad9-fc401f9a396d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3 - Review Evaluation Results\n",
    "\n",
    "Review the evaluation results in the **Experiments** section. Examine the following information regarding this evaluation:\n",
    "\n",
    "- Token usage\n",
    "\n",
    "- Model metrics\n",
    "\n",
    "- Results of the custom metric defined earlier (\"readability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b80cb26b-96cb-4d1e-a0ab-fa0aa6ba4b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you evaluated a RAG chain using the Mosaic AI Evaluation Framework library. You began by loading the dataset and RAG model. Then, you defined a custom metric and initiated the evaluation process. Finally, you reviewed the results through the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a327927-9d47-45cd-84e8-7d4497ebfd46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.LAB - Evaluation with Mosaic AI Agent Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}