{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f77da81d-b5a8-49e0-9292-30a157025b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfd11e48-2c7f-410e-b94e-c9602ef0b5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# LAB - Assembling a RAG Application\n",
    "\n",
    "In this lab, we will assemble a Retrieval-augmented Generation (RAG) application using the components we previously created. The primary goal is to create a seamless pipeline where users can ask questions, and our system retrieves relevant documents from a Vector Search index to generate informative responses.\n",
    "\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "In this lab, you will need to complete the following tasks;\n",
    "\n",
    "* **Task 1 :** Setup the Retriever Component\n",
    "\n",
    "* **Task 2 :** Setup the Foundation Model\n",
    "\n",
    "* **Task 3 :** Assemble the Complete RAG Solution\n",
    "\n",
    "* **Task 4 :** Save the Model to Model Registry in Unity Catalog\n",
    "\n",
    "**\uD83D\uDCDD Your task:** Complete the **`<FILL_IN>`** sections in the code blocks and follow the other steps as instructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a5052ad-1886-4b5d-ae34-29f50c1064f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f4010ba-6023-4f3b-9efb-e7b3c65ad88b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **15.4.x-cpu-ml-scala2.12**\n",
    "\n",
    "**\uD83D\uDEA8 Important:** This lab relies on the resources established in the previous one. Please ensure you have completed the prior lab before starting this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4175336c-beb6-46cc-825e-0fef9803dbb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup\n",
    "\n",
    "Before starting the lab, run the provided classroom setup script. This script will define configuration variables necessary for the lab. Execute the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2458b879-46d8-4a9b-b8f7-afdf7954e36f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\njupyter-server 1.23.4 requires anyio<4,>=3.1.0, but you have anyio 4.10.0 which is incompatible.\nlangchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.63 which is incompatible.\nnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.11.7 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qq databricks-vectorsearch langchain==0.3.7 flashrank langchain-databricks PyPDF2\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9747681b-29b6-49c3-bc4c-036f9fda8663",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e25425f-3378-4975-87b8-0f04b6a4d9e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Other Conventions:**\n",
    "\n",
    "Throughout this demo, we'll refer to the object `DA`. This object, provided by Databricks Academy, contains variables such as your username, catalog name, schema name, working directory, and dataset locations. Run the code block below to view these details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec392161-d2a5-4f2f-80c8-71ecd555d99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Username:          labuser11131632_1754970105@vocareum.com\nCatalog Name:      dbacademy\nSchema Name:       labuser11131632_1754970105\nWorking Directory: /Volumes/dbacademy/ops/labuser11131632_1754970105@vocareum_com\nDataset Location:  NestedNamespace (arxiv='/Volumes/dbacademy_arxiv/v01', dais='/Volumes/dbacademy_dais/v01', news='/Volumes/dbacademy_news/v01', docs='/Volumes/dbacademy_docs/v01')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Username:          {DA.username}\")\n",
    "print(f\"Catalog Name:      {DA.catalog_name}\")\n",
    "print(f\"Schema Name:       {DA.schema_name}\")\n",
    "print(f\"Working Directory: {DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:  {DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "184cee51-fa47-4606-8e20-c63a2ee794da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Setup the Retriever Component\n",
    "**Steps:**\n",
    "1. Define the embedding model.\n",
    "1. Get the vector search index that was created in the previous lab.\n",
    "1. Generate a **retriever** from the vector store. The retriever should return **three results.**\n",
    "1. Write a test prompt and show the returned search results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958b06d3-debe-4375-8b77-20a2bf4e8134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned Vector Search endpoint name: vs_endpoint_2.\n"
     ]
    }
   ],
   "source": [
    "## Components we created before\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "vs_endpoint_name = vs_endpoint_prefix+str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "print(f\"Assigned Vector Search endpoint name: {vs_endpoint_name}.\")\n",
    "\n",
    "vs_index_fullname = f\"{DA.catalog_name}.{DA.schema_name}.lab_pdf_text_managed_vs_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "351a116b-60b8-44a4-a6c1-28b91d993aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant documents: [Document(metadata={'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2303.10130.pdf'}, page_content='(Autoretal.,2022a)findsas\\nwell that automation and augmentation exposures tend to be positively correlated. There is also a growing set\\nof studies examining specific economic impacts and opportunities for LLMs (Bommasani et al., 2021; Felten\\net al., 2023; Korinek, 2023; Mollick and Mollick, 2022; Noy and Zhang, 2023; Peng et al., 2023). Alongside\\nthis work, our measurements help characterize the broader potential relevance of language models to the\\nlabor market.\\nGeneral-purpose technologies (e.g. printing, the steam engine) are characterized by widespread prolifera-\\ntion, continuous improvement, and the generation of complementary innovations (Bresnahan and Trajtenberg,\\n1995; Lipsey et al., 2005). Their far-reaching consequences, which unfold over decades, are difficult to\\nanticipate,particularlyinrelationtolabordemand(Bessen,2018;KorinekandStiglitz,2018;Acemogluetal.,\\nWORKING PAPER\\n2020; Benzell et al., 2021). The realization of general purpose technologies’ full potential requires extensive\\nco-invention(BresnahanandTrajtenberg,1995;Bresnahanetal.,1996,2002;Lipseyetal.,2005;Dixonetal.,\\n2021),acostlyandtime-consumingprocessinvolvingthediscoveryofnewbusinessprocedures(David,1990;\\nBresnahan, 1999; Frey, 2019; Brynjolfsson et al., 2021; Feigenbaum and Gross, 2021).'), Document(metadata={'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2303.10130.pdf'}, page_content='Collectively, these characteristics imply that Generative Pre-trained Transformers (GPTs) are general-purpose\\ntechnologies (GPTs). 4(Bresnahan and Trajtenberg, 1995; Lipsey et al., 2005).\\n(Goldfarb et al., 2023) argue that machine learning as a broad category is likely a general-purpose\\ntechnology. Ourevidencesupportsawiderimpact,asevensubsetsofmachinelearningsoftwaremeetthe\\ncriteriaforgeneral-purposetechnologystatusindependently. Thispaper’sprimarycontributionsaretoprovide\\nasetofmeasurementsofLLMimpactpotentialandtodemonstratetheusecaseofapplyingLLMstodevelop\\nsuchmeasurementsefficientlyandatscale. Additionally,weshowcasethegeneral-purposepotentialofLLMs.\\nIf \"GPTs are GPTs,\" the eventual trajectory of LLM development and application may be challenging for\\n3Baumol’s cost disease is a theory that explains why the cost of labor-intensive services, such as healthcare and education,\\nincreases over time. This happens because wages for skilled workers in other industries increase, but there is no corresponding\\nincreaseinproductivityorefficiencyintheseserviceindustries. Therefore,thecostoflaborintheseindustriesbecomesrelatively\\nmore expensive compared to other goods and services in the economy.\\n4For the remainder of the paper we spell out general-purpose technologies when it is used outside of stating \"GPTs are GPTs.\"\\nWORKING PAPER\\npolicymakers to predict and regulate. As with other general-purpose technologies, much of these algorithms’\\npotential will emerge across a broad range of economically valuable use cases, including the creation of new\\ntypesofwork(AcemogluandRestrepo,2018;Autoretal.,2022a).')]\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-6b8ca4c6646c442899d54b7d55a8b3f1\"",
      "text/plain": [
       "Trace(request_id=tr-6b8ca4c6646c442899d54b7d55a8b3f1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_databricks import DatabricksEmbeddings\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.docstore.document import Document\n",
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "def get_retriever(cache_dir=f\"{DA.paths.working_dir}/opt\"):\n",
    "\n",
    "    def retrieve(query, k: int=10):\n",
    "        if isinstance(query, dict):\n",
    "            query = next(iter(query.values()))\n",
    "\n",
    "        ## get the vector search index\n",
    "        vsc = VectorSearchClient(disable_notice=True)\n",
    "        vs_index = vsc.get_index(endpoint_name=vs_endpoint_name, index_name=vs_index_fullname)\n",
    "        \n",
    "        ## get similar k documents\n",
    "        return query, vs_index.similarity_search(\n",
    "            query_text=query,\n",
    "            columns=[\"pdf_name\", \"content\"],\n",
    "            num_results=k)\n",
    "\n",
    "    def rerank(query, retrieved, cache_dir, k: int=2):\n",
    "        ## format result to align with reranker lib format \n",
    "        passages = []\n",
    "        for doc in retrieved.get(\"result\", {}).get(\"data_array\", []):\n",
    "            new_doc = {\"file\": doc[0], \"text\": doc[1]}\n",
    "            passages.append(new_doc)       \n",
    "        ## Load the flashrank ranker\n",
    "        ranker = Ranker(model_name=\"rank-T5-flan\", cache_dir=cache_dir)\n",
    "\n",
    "        ## rerank the retrieved documents\n",
    "        rerankrequest = RerankRequest(query=query, passages=passages)\n",
    "        results = ranker.rerank(rerankrequest)[:k]\n",
    "\n",
    "        ## format the results of rerank to be ready for prompt\n",
    "        return [Document(page_content=r.get(\"text\"), metadata={\"source\": r.get(\"file\")}) for r in results]\n",
    "\n",
    "    ## the retriever is a runnable sequence of retrieving and reranking.\n",
    "    return RunnableLambda(retrieve) | RunnableLambda(lambda x: rerank(x[0], x[1], cache_dir))\n",
    "\n",
    "## test our retriever\n",
    "question = {\"input\": \"How does Generative AI impact humans?\"}\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.invoke(question)\n",
    "print(f\"Relevant documents: {similar_documents}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d3436da-c718-423c-94d2-6ac763a159ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Setup the Foundation Model\n",
    "**Steps:**\n",
    "1. Define the foundation model for generating responses. Use `llama-3.3` as foundation model. \n",
    "2. Test the foundation model to ensure it provides accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192755d6-1323-43a8-bae3-9182a2c15c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.ipykernel/8319/command-2179535239131770-3769911284:5: LangChainDeprecationWarning: Use databricks_langchain.ChatDatabricks\n  chat_model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 300)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test chat model: content='Generative AI refers to a type of artificial intelligence that is capable of generating new, original content, such as images, videos, music, text, or even entire datasets. This is in contrast to traditional AI, which is typically focused on analyzing and processing existing data.\\n\\nGenerative AI uses complex algorithms and machine learning techniques, such as deep learning and neural networks, to learn patterns and structures in data and then generate new data that is similar in style, structure, and content. The goal of generative AI is to create new, synthetic data that is indistinguishable from real data.\\n\\nThere are several types of generative AI, including:\\n\\n1. **Generative Adversarial Networks (GANs)**: GANs consist of two neural networks that work together to generate new data. One network generates new data, while the other network evaluates the generated data and tells the generator whether it is realistic or not.\\n2. **Variational Autoencoders (VAEs)**: VAEs are a type of neural network that learns to compress and reconstruct data. They can be used to generate new data by sampling from the compressed representation.\\n3. **Recurrent Neural Networks (RNNs)**: RNNs are a type of neural network that is well-suited for generating sequential data, such as text or music.\\n4. **Transformers**: Transformers are a type of neural network that is particularly well-suited for generating text and other sequential data.\\n\\nGenerative AI has many potential' additional_kwargs={} response_metadata={'prompt_tokens': 16, 'completion_tokens': 300, 'total_tokens': 316} id='run--00248426-2351-4457-8754-9af849ecf198-0'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-f069a762817c490f9179ea45974e2763\"",
      "text/plain": [
       "Trace(request_id=tr-f069a762817c490f9179ea45974e2763)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "from langchain_databricks import ChatDatabricks\n",
    "\n",
    "## define foundation model for generating responses\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 300)\n",
    "\n",
    "## test foundation model\n",
    "print(f\"Test chat model: {chat_model.invoke('What is Generative AI?')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bdbf331-8766-4f7d-a6a2-ed224346e3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 3: Assemble the Complete RAG Solution\n",
    "**Steps:**\n",
    "1. Merge the retriever and foundation model into a single Langchain chain.\n",
    "2. Configure the Langchain chain with proper templates and context for generating responses.\n",
    "3. Test the complete RAG solution with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c769ba-b708-4749-bb1e-1ca7645d7cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'How Generative AI impacts humans?', 'context': [{'metadata': {'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2303.10130.pdf'}, 'page_content': 'arXivpreprint\\narXiv:2302.07842.\\nMoll,B.,Rachel,L.,andRestrepo,P.(2021). Unevengrowth: Automation’simpactonincomeandwealth\\ninequality. SSRNElectronic Journal.\\nMollick,E.R.andMollick,L.(2022). Newmodesoflearningenabledbyaichatbots: Threemethodsand\\nassignments. Available atSSRN.\\nNoy, S. and Zhang, W. (2023). Experimental evidence on the productivity effects of generative artificial\\nintelligence. Available atSSRN4375283.\\nO*NET (2023). O*net 27.2 database.\\nOpenAI (2022). Introducing chatgpt.\\nOpenAI (2023a). Gpt-4 system card. Technical report, OpenAI.\\nOpenAI (2023b). Gpt-4 technical report. Technical report, OpenAI.\\nOuyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.L.,Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,\\nK.,Ray,A.,etal.(2022). Traininglanguagemodelstofollowinstructionswithhumanfeedback. arXiv\\npreprintarXiv:2203.02155.\\nPeng,S.,Kalliamvakou,E.,Cihon,P.,andDemirer,M.(2023). Theimpactofaiondeveloperproductivity:\\nEvidence from github copilot. arXivpreprintarXiv:2302.06590.'}, {'metadata': {'source': 'dbfs:/Volumes/dbacademy_arxiv/v01/arxiv-articles/2303.10130.pdf'}, 'page_content': 'Mostothertechnology exposuremeasureswe\\nexamine are statistically significantly correlated with our preferred exposure measure, while measures of\\nmanualroutinenessandroboticsexposureshownegativecorrelations. Thevarianceexplainedbytheseearlier\\nefforts (Acemogluand Autor, 2011a;Frey and Osborne,2017; Brynjolfsson et al.,2018; Felten etal., 2018;\\nWebb, 2020; Brynjolfsson et al., 2023), along with wage controls, ranges from 60 to 72%, indicating that 28\\nto40%ofthevariationinourAIexposuremeasureremainsunaccountedforbyprevioustechnologyexposure\\nmeasurements.\\nWeanalyzeexposurebyindustryanddiscoverthatinformationprocessingindustries(4-digitNAICS)\\nexhibit high exposure, while manufacturing, agriculture, and mining demonstrate lower exposure. The\\nconnectionbetweenproductivitygrowthinthepastdecadeandoverallLLMexposureappearsweak,suggesting\\na potential optimistic case that future productivity gains from LLMs may not exacerbate possible cost disease\\neffects (Baumol, 2012; Aghion et al., 2018). 3\\nOur analysis indicates that the impacts of LLMs like GPT-4, are likely to be pervasive. While LLMs\\nhave consistently improved in capabilities over time, their growing economic effect is expected to persist and\\nincrease even if we halt the development of new capabilities today. We also find that the potential impact of\\nLLMs expands significantly when we take into account the development of complementary technologies.\\nCollectively, these characteristics imply that Generative Pre-trained Transformers (GPTs) are general-purpose\\ntechnologies (GPTs).'}], 'answer': 'Generative AI is likely to have a pervasive impact on humans, with potential effects on income and wealth inequality, productivity, and job displacement, particularly in industries with high exposure to automation, such as information processing.'}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-403feadd0fbb466796e98edf95b4a2fe\"",
      "text/plain": [
       "Trace(request_id=tr-403feadd0fbb466796e98edf95b4a2fe)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "## define template for prompt\n",
    "TEMPLATE = \"\"\"You are an assistant for GENAI teaching class. You are answering questions related to Generative AI and how it impacts humans life. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "Use the following pieces of context to answer the question at the end:\n",
    "{context}\n",
    "Question: {input}\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"input\"])\n",
    "\n",
    "## unwrap the longchain document from the context to be a dict so we can register the signature in mlflow\n",
    "def unwrap_document(answer):\n",
    "  return answer | {\"context\": [{\"metadata\": r.metadata, \"page_content\": r.page_content} for r in answer['context']]}\n",
    "\n",
    "## merge retriever and foundation model into Langchain chain\n",
    "question_answer_chain = create_stuff_documents_chain(chat_model, prompt)\n",
    "chain = create_retrieval_chain(get_retriever(), question_answer_chain)|RunnableLambda(unwrap_document)\n",
    "\n",
    "\n",
    "## test the complete RAG solution with sample query\n",
    "question = {\"input\": \"How Generative AI impacts humans?\"}\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d30e565-f540-4a3a-acec-bab0cc3bf313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Task 4: Save the Model to Model Registry in Unity Catalog\n",
    "**Steps:**\n",
    "1. Register the assembled RAG model in the Model Registry with Unity Catalog.\n",
    "2. Ensure that all necessary dependencies and requirements are included.\n",
    "3. Provide an input example and infer the signature for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d19da7b-12e4-4cc4-a8e1-c82ec61d361c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-385a80ad-7517-44df-9981-f963bf86b882/lib/python3.11/site-packages/langchain/chains/api/base.py:56: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n\nFor example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\nwith: `from pydantic import BaseModel`\nor the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n\n  from langchain_community.utilities.requests import TextRequestsWrapper\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-385a80ad-7517-44df-9981-f963bf86b882/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:918: UserWarning: Mixing V1 models and V2 models (or constructs, like `TypeAdapter`) is not supported. Please upgrade `TextRequestsWrapper` to V2.\n  warn(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-385a80ad-7517-44df-9981-f963bf86b882/lib/python3.11/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n* 'allow_population_by_field_name' has been renamed to 'validate_by_name'\n  warnings.warn(message, UserWarning)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-385a80ad-7517-44df-9981-f963bf86b882/lib/python3.11/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n* 'underscore_attrs_are_private' has been removed\n  warnings.warn(message, UserWarning)\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-385a80ad-7517-44df-9981-f963bf86b882/lib/python3.11/site-packages/mlflow/langchain/runnables.py:292: UserWarning: Your model contains a class imported from the LangChain partner package `langchain-databricks`. When loading the model back, MLflow will use the community version of the classes instead of the partner packages, which may lead to unexpected behavior. To ensure that the model is loaded correctly, it is recommended to save the model with the 'model-from-code' method instead: https://mlflow.org/docs/latest/models.html#models-from-code\n  warnings.warn(\n2025/08/12 04:39:42 INFO mlflow: Attempting to auto-detect Databricks resource dependencies for the current langchain model. Dependency auto-detection is best-effort and may not capture all dependencies of your langchain model, resulting in authorization errors when serving or querying your model. We recommend that you explicitly pass `resources` to mlflow.langchain.log_model() to ensure authorization to dependent resources succeeds when the model is deployed.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4209782f3d4f45b5b2574786f6b33e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'dbacademy.labuser11131632_1754970105.rag_app_lab_4'.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3f83f64cd840c584905093b393ddf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '1' of model 'dbacademy.labuser11131632_1754970105.rag_app_lab_4'.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "## import necessary libraries\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "## set Model Registry URI to Unity Catalog\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f\"{DA.catalog_name}.{DA.schema_name}.rag_app_lab_4\"\n",
    "\n",
    "## register the assembled RAG model in Model Registry with Unity Catalog\n",
    "with mlflow.start_run(run_name=\"rag_app_lab_4\") as run:\n",
    "    signature = infer_signature(question, answer)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever,\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow.__version__,\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        input_example=question,\n",
    "        signature=signature\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d0d9ce-dfcb-4a94-a88f-686665f5ffdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Clean up Resources\n",
    "\n",
    "This was the final lab. You can delete all resources created in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c47fcea0-5830-4c75-a649-a7a11075a01f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this lab, you learned how to assemble a Retrieval-augmented Generation (RAG) application using Databricks components. By integrating Vector Search for document retrieval and a foundational model for response generation, you created a powerful tool for answering user queries. This lab provided hands-on experience in building end-to-end AI applications and demonstrated the capabilities of Databricks for natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "597e3b8b-7e38-4502-91c3-22c74ca2f443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4.LAB - Assembling a RAG Application",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}