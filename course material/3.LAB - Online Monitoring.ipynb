{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "783af626-4f40-488a-bc8b-8aec565b93d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de83315b-915f-491d-93f8-c48aebcd750c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LAB - Online Monitoring\n",
    "\n",
    "In this lab, you will create an online monitor for a sample inference table using Databricks Lakehouse Monitoring. A sample inference table, extracted from a deployed Model Serving Endpoint, has been imported for you to use for monitoring.\n",
    "\n",
    "**Lab Outline:**\n",
    "\n",
    "*In this lab, you will need to complete the following tasks:*\n",
    "\n",
    "* **Task 1:** Define Evaluation Metrics\n",
    "* **Task 2:** Unpack the Request Payload\n",
    "* **Task 3:** Compute Metrics\n",
    "* **Task 4:** Save the Processed Inference Table\n",
    "* **Task 5:** Create a Monitor on the Inference Table\n",
    "* **Task 6:** Review the Monitor Details\n",
    "* **Task 7:** View the Monitor Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b0a9796-ec75-4c15-8e87-f667ecfaeccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "2. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "   - Click **More** in the drop-down.\n",
    "   \n",
    "   - In the **Attach to an existing compute resource** window, use the first drop-down to select your unique cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "2. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "3. Wait a few minutes for the cluster to start.\n",
    "\n",
    "4. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7dd6d956-b4e7-4ff3-a4cd-213a728ad2ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "Please review the following requirements before starting the lesson:\n",
    "\n",
    "* To run this notebook, you need to use one of the following Databricks runtime(s): **15.4.x-cpu-ml-scala2.12**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf55e30f-1973-4c96-9d8b-de0532fb5551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Classroom Setup\n",
    "\n",
    "Install required libraries and load classroom configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5768daa-c184-472d-b026-a846109faa69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qq databricks-sdk textstat tiktoken evaluate\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6ed453-c922-4ae4-a65a-75c65fee2628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nThe examples and models presented in this course are intended solely for demonstration and educational purposes.\n Please note that the models and prompt examples may sometimes contain offensive, inaccurate, biased, or harmful content.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup-03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9e56659-aa0d-470d-a25f-55f909021087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inference Table\n",
    "\n",
    "You are going to use the same inference table that we used for the demo. The inference table is pre-loaded and ready to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2064de21-e411-472e-8ea6-01284cab9100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>client_request_id</th><th>databricks_request_id</th><th>date</th><th>timestamp_ms</th><th>status_code</th><th>execution_time_ms</th><th>request</th><th>response</th><th>sampling_fraction</th><th>request_metadata</th></tr></thead><tbody><tr><td>null</td><td>5dca1319-2a0d-4d82-bbac-a10bde2c2e84</td><td>2025-08-03</td><td>1754174434710</td><td>200</td><td>3066</td><td>{\"inputs\": [{\"query\": \"Can you discuss the main challenges and opportunities for future research of very large models in the big data era, as mentioned in the text?\"}]}</td><td>{\"predictions\": [\"Sure, I'd be happy to discuss the main challenges and opportunities for future research of very large models in the big data era.\\n\\nChallenges:\\n\\n1. Limited context size: Even though the size of language models has been increasing, most models still use a relatively small context size compared to the potentially large context needed to correctly perform language modeling. This can lead to issues when the necessary knowledge is not present in the context.\\n2. Storage of knowledge: Massive scale is required to store knowledge that is not present in the context but necessary to perform the task at hand. This can be a significant challenge in terms of both storage and computation.\\n3. Non-parametric models: By equipping language models with modules that retrieve information from external databases, it is possible to match certain capabilities of some of the largest language models while having fewer parameters. However, this results in non-parametric models, which can introduce new challenges in terms of data management and retrieval.\\n\\nOpportunities:\\n\\n1. Improved context relevance: By increasing the relevance of the context provided to language models, it is possible to improve their performance on certain tasks. This can be achieved through strategies such as adding information extracted from relevant external documents or allowing language models to leverage external tools.\\n2. Reasoning strategies: Language models can improve their context via reasoning strategies, which can lead to more relevant contexts and improved performance on certain tasks.\\n3. External tools: Allowing language models to leverage external tools can augment the current context with important missing information that\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>b66fd6af-c9a4-4672-ba39-9853a8ade8d4</td><td>2025-08-03</td><td>1754176878826</td><td>200</td><td>3341</td><td>{\"inputs\": [{\"query\": \"How does the study's exposure measure reflect the technical capacity to make human labor more efficient, and what factors imply that technical feasibility does not guarantee labor productivity or automation outcomes?\"}]}</td><td>{\"predictions\": [\"The study's exposure measure reflects the technical capacity to make human labor more efficient by quantifying the exposure of different occupations to large language models (LLMs) like GPT-4. This measure is based on the assumption that as LLMs improve, they will be able to perform an increasing number of tasks currently done by humans, thereby making human labor more efficient.\\n\\nHowever, there are several factors that imply technical feasibility does not guarantee labor productivity or automation outcomes. First, the study finds that the AI Occupational Exposure Score is not correlated with the study's exposure measures, suggesting that the potential impact of LLMs on labor productivity and automation may not be fully captured by existing measures of AI exposure.\\n\\nSecond, the study finds that manual work is not exposed to LLMs or even LLMs with additional systems integration for the time being. This suggests that the technical feasibility of automating manual tasks with LLMs may not be sufficient to guarantee labor productivity or automation outcomes, as there may be other factors such as the cost of implementation or the availability of complementary technologies that also need to be considered.\\n\\nFinally, the study notes that the potential impact of LLMs expands significantly when complementary technologies are taken into account. This suggests that the technical feasibility of automating tasks with LLMs may depend on the development of other technologies, and that the relationship between technical feasibility and labor productivity or automation outcomes may be complex and multifaceted.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>b4bb72de-bd46-4f73-91f1-4d5774b56c08</td><td>2025-08-03</td><td>1754176880893</td><td>200</td><td>1968</td><td>{\"inputs\": [{\"query\": \"How do human assessments compare to GPT-4 classifications in the study's primary exposure dataset, and what is the main finding regarding the potential impact of LLMs like GPT-4 when considering both current model capabilities and anticipated tools built upon them?\"}]}</td><td>{\"predictions\": [\"Human assessments and GPT-4 classifications show a high degree of agreement in the primary exposure dataset. The main finding regarding the potential impact of LLMs like GPT-4 suggests that while current model capabilities are significant, the impact on occupations may be even greater when considering anticipated tools built upon them. This is because LLMs have the potential to automate a wide range of tasks, including those that are currently considered non-automatable. However, the study also notes some limitations, such as the validity of the task-based framework and the lack of expertise and task interpretation by human annotators.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>199e33d2-6ead-4a70-88af-2c0c29cf5a7e</td><td>2025-08-03</td><td>1754176884462</td><td>200</td><td>3408</td><td>{\"inputs\": [{\"query\": \"What are the key findings regarding the relationship between exposure to LLMs and various factors such as wages, skillsets, and job preparation difficulties, and how do these findings compare to previous efforts documenting the distribution of automation exposure in the economy?\"}]}</td><td>{\"predictions\": [\"1. Relationship between LLM exposure and wages: The study finds that occupational exposure to LLMs weakly increases with the difficulty of job preparation, but the relationship between LLM exposure and wages is not explicitly mentioned. It suggests that workers facing higher barriers to entry in their jobs tend to experience more exposure to LLMs.\\n\\n2. Skillsets and LLM exposure: The study does not provide a direct comparison between skillsets and LLM exposure. However, it mentions that LLMs like GPT-4 can assess tasks for exposure and automation potential, which could be used to evaluate skillsets in the future.\\n\\n3. Comparison to previous automation exposure efforts: The study's measurements are broadly consistent with previous efforts documenting the distribution of automation exposure in the economy. Most other technology exposure measures examined in the study are statistically significantly correlated with the preferred exposure measure, while measures of manual routineness and robotics exposure show negative correlations.\\n\\n4. Industry-specific LLM exposure: Information processing industries exhibit high exposure to LLMs, while manufacturing, agriculture, and mining demonstrate lower exposure.\\n\\n5. Productivity growth and LLM exposure: The connection between productivity growth in the past decade and overall LLM exposure appears weak, suggesting a potential optimistic case that future productivity gains from LLMs may not exacerbate possible cost disease effects.\\n\\n6. Pervasive impacts of LLMs: The study indicates that the impacts of LLMs are likely to be pervasive, with the potential impact expanding significantly when\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>ee3ba5f7-376d-436d-a816-4077a63690c8</td><td>2025-08-03</td><td>1754175288565</td><td>200</td><td>3723</td><td>{\"inputs\": [{\"query\": \"Can you discuss the history and evolution of PFMs in the NLP, CV, and Graph Learning (GL) domains?\"}]}</td><td>{\"predictions\": [\"Sure, I'd be happy to discuss the history and evolution of Pretrained Foundation Models (PFMs) in the Natural Language Processing (NLP), Computer Vision (CV), and Graph Learning (GL) domains.\\n\\nIn the NLP domain, PFMs have evolved significantly over the years. Traditional models focused on static feature learning, but the introduction of the Transformer model revolutionized the field. The Transformer, which relies solely on attention mechanisms and doesn't use recurrent or convolutional architectures, allowed for the transfer of weighted representation knowledge between various neural units. This led to the development of models like GPT-3, a generative model based on the Transformer.\\n\\nIn the CV domain, the Vision Transformer (ViT) was proposed to represent an image as a series of image patches, similar to a series of word embeddings. This was a significant evolution from traditional CV models that relied on convolutional neural networks.\\n\\nIn the GL domain, Graph Transformer Networks (GTN) were employed to learn new graph structures and powerful node representations without domain knowledge. This was a significant advancement from traditional GL models that relied on graph convolutional networks.\\n\\nIn summary, the evolution of PFMs in these three domains has been marked by a shift from static feature learning to dynamic feature learning, and the Transformer model has played a crucial role in this evolution. The Transformer's ability to facilitate the transfer of weighted representation knowledge between various neural units has made it a key component in PFMs in NLP, CV, and GL.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>d0aa9544-4183-45ae-954e-4cff1b5c05e8</td><td>2025-08-03</td><td>1754174131304</td><td>200</td><td>3996</td><td>{\"inputs\": [{\"query\": \"What are the major advantages of PFMs, and how have they inspired research in model efficiency, security, and compression?\"}]}</td><td>{\"predictions\": [\"1. Major Advantages of PFMs:\\nPFMs (Pretrained Foundation Models) have several major advantages:\\n   a. Versatility: PFMs can be adapted to a wide range of tasks and domains, including natural language processing (NLP), computer vision (CV), and graph learning (GL).\\n   b. Improved Performance: PFMs have shown significant improvements in performance across various tasks compared to models trained from scratch.\\n   c. Transfer Learning: PFMs enable transfer learning, where a model pretrained on a large dataset can be fine-tuned for a specific task with less data and computation.\\n\\n2. Inspiration in Model Efficiency, Security, and Compression:\\nThe success of PFMs has inspired research in model efficiency, security, and compression:\\n   a. Model Efficiency: Researchers are exploring ways to make PFMs more efficient, such as developing more efficient model architectures, optimizing training algorithms, and reducing the computational requirements for fine-tuning.\\n   b. Security: PFMs have raised concerns about security and privacy, leading to research on secure and private model training, such as federated learning and differential privacy.\\n   c. Compression: To make PFMs more practical for real-world applications, researchers are working on model compression techniques, such as pruning, quantization, and knowledge distillation, to reduce the size and computational requirements of PFMs.\\n\\nIn summary, PFMs have shown significant potential in various domains, and their success has inspired research in model efficiency, security, and compression to make them more\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>fafeadc1-1fd3-4fe5-a2f7-b446486ec53a</td><td>2025-08-03</td><td>1754173582302</td><td>200</td><td>3636</td><td>{\"inputs\": [{\"query\": \"How does the pretraining technique used in PFMs differ between the NLP, CV, and GL domains?\"}]}</td><td>{\"predictions\": [\"In the NLP domain, pretraining techniques used in PFMs involve training a general model on large amounts of unlabeled text data, which can then be fine-tuned for various downstream tasks. This approach allows well-trained language models to capture rich knowledge, such as long-term dependencies and hierarchical relationships, that can be beneficial for downstream tasks. Early pretraining techniques in NLP were static, but dynamic pretraining techniques, such as BERT and XLNet, have since been proposed to better adapt to different semantic environments.\\n\\nIn the CV domain, pretraining techniques involve training a model on a large dataset of images, such as ImageNet, and then fine-tuning it for specific tasks, such as image classification or object detection. Pretrained models in CV can capture features such as shapes, textures, and colors, which can be useful for various computer vision tasks.\\n\\nIn the GL domain, pretraining techniques involve training a model on a large graph dataset, such as citation networks or social networks, and then fine-tuning it for specific tasks, such as node classification, link prediction, or graph classification. Pretrained models in GL can capture structural and relational information in graphs, which can be useful for various graph-based tasks.\\n\\nOverall, while the specific pretraining techniques used in PFMs may differ between the NLP, CV, and GL domains, they all involve training a general model on large amounts of data and then fine-tuning it for specific tasks. The main difference lies in the type of\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>a8b2637e-93fe-429f-908a-adf2e7da50ce</td><td>2025-08-03</td><td>1754176635195</td><td>200</td><td>2319</td><td>{\"inputs\": [{\"query\": \"How does the proposed assessing LLM capabilities and their potential effects on jobs complement predictions of technology's impacts on work, and what specific criteria does it include for measuring exposure to LLMs?\"}]}</td><td>{\"predictions\": [\"The proposed rubric for assessing LLM capabilities and their potential effects on jobs complements predictions of technology's impacts on work by providing a framework to understand the evolving landscape of language models and their associated technologies. It measures the overall exposure of tasks to LLMs, which serves as a proxy for potential economic impact without distinguishing between labor-augmenting or labor-displacing effects.\\n\\nThe specific criteria included in the rubric for measuring exposure to LLMs are:\\n\\n1. The extent to which the task can be performed using text-based inputs and outputs.\\n2. The level of creativity, critical thinking, and complex communication required for the task.\\n3. The degree to which the task can be broken down into smaller, repeatable steps.\\n4. The availability and quality of data required to train LLMs for the task.\\n5. The potential for integrating LLMs with complementary technologies to enhance their capabilities.\\n\\nThese criteria help determine the level of exposure a task has to LLMs, considering both the current capabilities of LLMs and anticipated LLM-powered software.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>c2339dd8-edfe-4013-a33c-a047a7a124f5</td><td>2025-08-03</td><td>1754176326963</td><td>200</td><td>1488</td><td>{\"inputs\": [{\"query\": \"How does CaP compare against baselines in simulated language-instructed manipulation tasks?\"}]}</td><td>{\"predictions\": [\"In the study, CaP (Language-Model-Assisted Policy Learning) was compared against several baselines in simulated language-instructed manipulation tasks. The results demonstrated that CaP outperformed the baselines in terms of success rate and sample efficiency. CaP was able to interpret natural language instructions, process perception outputs, and parameterize low-dimensional inputs to control primitives, which contributed to its superior performance. However, CaP is still limited by the scope of available perception APIs and control primitives, and it may struggle with significantly longer or more complex commands.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>b1d0fc49-143f-4836-9cf0-a29272a04427</td><td>2025-08-03</td><td>1754176332023</td><td>200</td><td>2901</td><td>{\"inputs\": [{\"query\": \"What are the limitations of CaP in the context of the experiments?\"}]}</td><td>{\"predictions\": [\"1. Limited Generalization to Unseen Tasks: The performance of CaP degrades when it encounters tasks with unseen attributes or instructions, as observed in the table-top manipulation domain. This is because end-to-end systems like CLIPort struggle to generalize to new tasks that require precise numerical spatial-geometric reasoning.\\n\\n2. Dependence on Perception APIs: CaP relies on object detection APIs for perception, which may limit its performance if the APIs are not accurate or if they fail to detect certain objects.\\n\\n3. Limited Ability to Perform Precise Numerical Computations: While CaP outperforms natural-language planners in tasks that require precise numerical spatial-geometric reasoning, it still has limitations in its ability to perform precise numerical computations compared to direct coding.\\n\\n4. Limited Ability to Parse Complex Language Commands: CaP may struggle to parse complex language commands that require multiple steps or that involve ambiguous or unclear instructions.\\n\\n5. Limited Ability to Maintain a Session: CaP relies on a session-based system to maintain context between language commands, but this system may have limitations in its ability to maintain context over long periods of time or in complex scenarios.\\n\\n6. Limited Ability to Generate New Functions: While CaP can generate new functions, this ability is limited by the complexity of the functions and the availability of APIs to perform the required actions.\\n\\n7. Limited Ability to Draw Entirely New Shapes: While CaP can draw entirely new shapes from language commands, this ability is limited by the complexity of the shapes\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>d16e7054-ebae-4e03-a215-82d0ea119468</td><td>2025-08-03</td><td>1754177126888</td><td>200</td><td>2124</td><td>{\"inputs\": [{\"query\": \"What is the significance of the study's findings regarding the potential impact of LLMs like GPT-4, and how do these findings relate to the concept of general-purpose technologies?\"}]}</td><td>{\"predictions\": [\"The study's findings suggest that LLMs, such as GPT-4, have the potential to significantly impact a wide range of occupations within the U.S. economy, which is a key attribute of general-purpose technologies. General-purpose technologies are characterized by their pervasiveness, improvement over time, and the development of significant co-invention and spillovers. The study's results indicate that LLMs could meet these criteria, as they can affect various tasks and occupations, potentially leading to widespread economic impacts. This is consistent with the idea that machine learning, as a broad category, is likely a general-purpose technology, and LLMs are a subset of this category that meet the criteria for general-purpose technology status independently.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>69da39b3-4d0d-4a54-ad4d-6195a40444f4</td><td>2025-08-03</td><td>1754175350239</td><td>200</td><td>1544</td><td>{\"inputs\": [{\"query\": \"What is the role of supervised fine-tuning (SFT) in instruction-aligning methods for language models (LMs)?\"}]}</td><td>{\"predictions\": [\"Supervised Fine-Tuning (SFT) is a technique used in instruction-aligning methods for language models (LMs) to help them follow human intents and generate meaningful outputs. In SFT, a pretrained LM is fine-tuned using high-quality annotated data in a supervised manner. This process involves creating a template with input-output pairs and an instruction, which guides the LM to generate the desired output for a given input. SFT can help improve the usefulness and harmlessness of LMs, making them more aligned with human intentions and less likely to produce harmful or inappropriate content.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>f4d7e853-3c16-466f-b9d9-7e3e6e40d8bd</td><td>2025-08-03</td><td>1754175415649</td><td>200</td><td>4757</td><td>{\"inputs\": [{\"query\": \"How does reinforcement learning (RL) help enhance various NLP tasks, and what are some potential risks associated with its use?\"}]}</td><td>{\"predictions\": [\"1. Reinforcement Learning (RL) in NLP: RL has been applied to enhance various Natural Language Processing (NLP) tasks such as machine translation, summarization, dialogue generation, image captioning, question generation, text-games, and more. RL treats language generation tasks as sequential decision-making problems, which helps optimize non-differentiable objectives. This method aligns large language models (LMs) with human preferences, leading to more accurate and contextually relevant responses.\\n\\n2. Potential Risks of RL in NLP:\\n\\n   a. Overfitting: There is a risk of overfitting to metrics that use neural networks, leading to nonsensical samples that score well on the metrics. This can result in LMs generating irrelevant or inappropriate responses, which may negatively impact user experience and trust.\\n\\n   b. Misalignment with Human Preferences: While RL can align LMs with human preferences, there is a risk of misalignment if the reward model is not accurately trained or if the human preferences are not accurately represented. This can lead to LMs generating responses that do not align with user intent or expectations.\\n\\n   c. Ethical and Social Risks: Ethical and social risks of harm from LMs are significant concerns in Supervised Fine-Tuning (SFT). RL can potentially exacerbate these risks if not properly managed, as LMs may generate responses that are biased, offensive, or harmful to certain groups or individuals.\\n\\nIn summary, RL can significantly enhance NLP\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>2f75b3fc-edb8-4ad7-a525-881e87cd2cf5</td><td>2025-08-03</td><td>1754174797439</td><td>200</td><td>1981</td><td>{\"inputs\": [{\"query\": \"What are Pretrained Foundation Models (PFMs) and how are they used in Artificial Intelligence (AI)?\"}]}</td><td>{\"predictions\": [\"Pretrained Foundation Models (PFMs) are AI models that are first trained on large-scale data and can be fine-tuned for various downstream tasks across different data modalities. They are called \\\"foundation\\\" models because they provide a strong base for a wide range of applications. PFMs have been used in natural language processing (NLP), computer vision (CV), and graph learning (GL) tasks, demonstrating great potential in learning feature representations. They are trained using pretraining techniques, which involve training a general model on large amounts of data and tasks, and then fine-tuning it for specific applications. PFMs have shown superior performance in tasks such as text classification, text generation, image classification, object detection, and graph classification. Examples of PFMs include BERT, ChatGPT, and GPT-4.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>0171a696-60e8-414b-9448-ce0715643388</td><td>2025-08-03</td><td>1754172631811</td><td>400</td><td>null</td><td>{\"inputs\": [{\"query\": \"What is PPO?\"}]}</td><td>{\"error_code\": \"BAD_REQUEST\", \"message\": \"1 tasks failed. Errors: {0: 'error: Exception(\\\\'Response content b\\\\\\\\\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\\\\\\\\\', status_code 403\\\\') Traceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 124, in issue_request\\\\n    response.raise_for_status()\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/requests/models.py\\\", line 1024, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://curriculum-dev.cloud.databricks.com/api/2.0/vector-search/endpoints/genai_vs_endpoint/indexes/amine_elhelou_ud8n_da.default.pdf_text_vs_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 326, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 299, in single_call_api\\\\n    response = self.lc_model(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\\\n    return self.invoke(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\\\n    self._call(inputs, run_manager=run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 141, in _call\\\\n    docs = self._get_docs(question, run_manager=_run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 221, in _get_docs\\\\n    return self.retriever.get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 323, in get_relevant_documents\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 316, in get_relevant_documents\\\\n    result = self._get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/vectorstores.py\\\", line 696, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 292, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 318, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/index.py\\\", line 223, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 127, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\', status_code 403\\\\n\\\\n request payload: {\\\\'query\\\\': \\\\'What is PPO?\\\\'}'}\"}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>173cc7ee-af54-4519-a299-e297537c29bb</td><td>2025-08-03</td><td>1754172666336</td><td>200</td><td>2000</td><td>{\"inputs\": [{\"query\": \"What is PPO?\"}]}</td><td>{\"predictions\": [\"PPO, or Proximal Policy Optimization, is a type of reinforcement learning algorithm that aims to address the challenges of policy gradient methods by using a trust region to improve the stability and reliability of the learning process. It is designed to optimize a policy while ensuring that the new policy does not deviate too much from the old one, which helps to prevent drastic changes in the policy that could lead to instability or divergence. In the context of generative AI, PPO can be used to train models to generate realistic and diverse samples by optimizing a reward function that measures the quality of the generated samples.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>cf717436-eb9e-4410-8d05-9f0d9abce40a</td><td>2025-08-03</td><td>1754172601224</td><td>400</td><td>null</td><td>{\"inputs\": [{\"query\": \"What is PPO?\"}]}</td><td>{\"error_code\": \"BAD_REQUEST\", \"message\": \"1 tasks failed. Errors: {0: 'error: Exception(\\\\'Response content b\\\\\\\\\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\\\\\\\\\', status_code 403\\\\') Traceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 124, in issue_request\\\\n    response.raise_for_status()\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/requests/models.py\\\", line 1024, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://curriculum-dev.cloud.databricks.com/api/2.0/vector-search/endpoints/genai_vs_endpoint/indexes/amine_elhelou_ud8n_da.default.pdf_text_vs_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 326, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 299, in single_call_api\\\\n    response = self.lc_model(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\\\n    return self.invoke(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\\\n    self._call(inputs, run_manager=run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 141, in _call\\\\n    docs = self._get_docs(question, run_manager=_run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 221, in _get_docs\\\\n    return self.retriever.get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 323, in get_relevant_documents\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 316, in get_relevant_documents\\\\n    result = self._get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/vectorstores.py\\\", line 696, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 292, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 318, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/index.py\\\", line 223, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 127, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\', status_code 403\\\\n\\\\n request payload: {\\\\'query\\\\': \\\\'What is PPO?\\\\'}'}\"}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>b39f15f9-bd44-4792-8f16-9a110308baec</td><td>2025-08-03</td><td>1754172824814</td><td>200</td><td>1305</td><td>{\"inputs\": [{\"query\": \"What is PPO?\"}]}</td><td>{\"predictions\": [\"PPO, or Proximal Policy Optimization, is a type of reinforcement learning algorithm that aims to address the challenges of policy gradient methods. It does this by optimizing a surrogate objective function, which is designed to avoid large policy updates that could negatively impact performance. In the context of Generative AI, PPO can be used to train models to generate human-like text by optimizing the likelihood of generating human demonstrations.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>450c0629-7dae-4db3-b584-f3f1002dc971</td><td>2025-08-03</td><td>1754174979763</td><td>200</td><td>2079</td><td>{\"inputs\": [{\"query\": \"Can you explain the concept of pretraining in the context of PFMs and its origins in transfer learning?\"}]}</td><td>{\"predictions\": [\"Pretraining in the context of PFMs (Pretrained Foundation Models) refers to the process of training a general model on large amounts of data and tasks, which can then be fine-tuned for specific downstream applications. The idea of pretraining originated from transfer learning in the field of computer vision (CV). It was later applied to the natural language processing (NLP) domain, where well-trained language models (LMs) can capture rich knowledge beneficial for downstream tasks. Pretraining techniques in NLP have the advantage of using unlimited amounts of unlabeled text corpus for training. The concept of pretraining has evolved over time, with early static techniques being replaced by dynamic methods like BERT and XLNet. Pretrained Foundation Models have demonstrated great potential in learning feature representations in various learning tasks, such as text classification, text generation, image classification, object detection, and graph classification.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>e26cc26f-e2d1-43c1-a9c2-0d0c885438aa</td><td>2025-08-03</td><td>1754175224681</td><td>200</td><td>2202</td><td>{\"inputs\": [{\"query\": \"What are some examples of PFMs in the field of Computer Vision (CV)?\"}]}</td><td>{\"predictions\": [\"Some examples of Pretrained Foundation Models (PFMs) in the field of Computer Vision (CV) include:\\n\\n1. ViT-22B: This is a transformer-based model with about 22 billion parameters, which has achieved groundbreaking capabilities in various CV tasks.\\n2. Convolutional Neural Networks (CNNs): CNNs are a type of PFMs that have been widely used in CV for tasks such as image classification, object detection, and semantic segmentation. Examples of CNN-based PFMs include AlexNet, VGG, GoogLeNet, ResNet, and DenseNet.\\n3. Visual Transformers: Visual transformers are a type of PFMs that have been developed based on the transformer architecture, which was originally designed for natural language processing tasks. Examples of visual transformers include ViT, DeiT, and Swin Transformer.\\n\\nThese PFMs have been pretrained on large-scale datasets and can be fine-tuned for various downstream tasks in CV, such as image classification, object detection, and semantic segmentation.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>473dd70f-a19d-4358-9a13-a0c051663766</td><td>2025-08-03</td><td>1754175899866</td><td>200</td><td>2033</td><td>{\"inputs\": [{\"query\": \"What is the concept of chain-of-thought (CoT) prompting, and how does it improve the reasoning ability of LLMs?\"}]}</td><td>{\"predictions\": [\"Chain-of-thought (CoT) prompting is a technique for improving the reasoning abilities of large language models (LLMs). It involves providing the model with a few examples of a task, including the inputs, intermediate reasoning steps, and the final output. This prompt serves as a guide for the model to follow when generating responses to new tasks. The CoT approach has been shown to outperform standard prompting methods, particularly when used with large-scale models such as LaMDA, PaLM, or GPT3. By providing a clear sequence of reasoning steps, CoT prompting helps LLMs to better understand and solve complex problems, even without additional training or fine-tuning.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>93c56e84-1682-43d7-b03a-b4312b74afc6</td><td>2025-08-03</td><td>1754175963224</td><td>200</td><td>3033</td><td>{\"inputs\": [{\"query\": \"How can CoT prompting be used in a practical example, and what are its benefits in improving the performance of LLMs on various reasoning tasks?\"}]}</td><td>{\"predictions\": [\"CoT prompting can be used in a practical example by providing a prompt to the model that outlines a multi-step problem. For instance, if you ask the model \\\"After selling 30 out of his 100 chickens and 10 out of his 20 pigs, how many animals does a farmer have left?\\\", the model would generate a sequence of intermediate reasoning steps, such as \\\"The farmer has 100-30=70 chickens remaining\\\" and \\\"The farmer has 20-10=10 pigs remaining,\\\" before generating the final answer, such as \\\"The farmer has 70+10=80 animals remaining.\\\"\\n\\nThe benefits of CoT prompting in improving the performance of LLMs on various reasoning tasks include:\\n\\n1. Enhancing the ability of language models to reason about complicated problems.\\n2. Demonstrating efficacy in improving the performance of LLMs on various reasoning tasks, such as arithmetic, symbolic reasoning, and common sense.\\n3. Being a promising technique that can enhance the ability of language models to reason about complicated problems.\\n4. Showing slightly more harmless compared to without CoT.\\n5. Being an emergent property of model scale, meaning it works better with larger and more powerful language models.\\n6. Being possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>71ce55ed-f385-4e2e-9167-571d4d8c1444</td><td>2025-08-03</td><td>1754176025164</td><td>200</td><td>1007</td><td>{\"inputs\": [{\"query\": \"How does hierarchical code generation perform on code-generation benchmarks such as RoboCodeGen and HumanEval?\"}]}</td><td>{\"predictions\": [\"Hierarchical code generation performs well on code-generation benchmarks such as RoboCodeGen and HumanEval. On RoboCodeGen, hierarchical code generation outperforms flat code generation, with larger models performing better. On HumanEval, hierarchical code generation also improves performance, with higher pass rates compared to flat code generation.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>47e9aa39-d1a3-497b-a98d-3ff0ad59627e</td><td>2025-08-03</td><td>1754175717547</td><td>200</td><td>1654</td><td>{\"inputs\": [{\"query\": \"How does GPT-4 utilize RLHF fine-tuning to improve the reliability, creativity, and capability of the model?\"}]}</td><td>{\"predictions\": [\"GPT-4 utilizes RLHF (Reinforcement Learning with Human Feedback) fine-tuning to improve the reliability, creativity, and capability of the model by first pretraining the model to predict the subsequent token in a document. Afterward, the model undergoes RLHF fine-tuning, which involves training the model using reinforcement learning, where the reward model or machine labeler calculates a reward based on the model's output. This reward is then used to update the model using the Proximal Policy Optimization (PPO) algorithm. This process helps the model to better handle more nuanced instructions, be more creative, and more reliable in its responses.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>663c4b7f-9304-4d68-bdf0-e3e0a8cc5796</td><td>2025-08-03</td><td>1754172487675</td><td>400</td><td>null</td><td>{\"inputs\": [{\"query\": \"What is PPO?\"}]}</td><td>{\"error_code\": \"BAD_REQUEST\", \"message\": \"1 tasks failed. Errors: {0: 'error: Exception(\\\\'Response content b\\\\\\\\\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\\\\\\\\\', status_code 403\\\\') Traceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 124, in issue_request\\\\n    response.raise_for_status()\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/requests/models.py\\\", line 1024, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://curriculum-dev.cloud.databricks.com/api/2.0/vector-search/endpoints/genai_vs_endpoint/indexes/amine_elhelou_ud8n_da.default.pdf_text_vs_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 326, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 299, in single_call_api\\\\n    response = self.lc_model(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\\\n    return self.invoke(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\\\n    self._call(inputs, run_manager=run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 141, in _call\\\\n    docs = self._get_docs(question, run_manager=_run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 221, in _get_docs\\\\n    return self.retriever.get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 323, in get_relevant_documents\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 316, in get_relevant_documents\\\\n    result = self._get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/vectorstores.py\\\", line 696, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 292, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 318, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/index.py\\\", line 223, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 127, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\', status_code 403\\\\n\\\\n request payload: {\\\\'query\\\\': \\\\'What is PPO?\\\\'}'}\"}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>e89a6539-5304-4726-927d-465b1c6724e0</td><td>2025-08-03</td><td>1754176328776</td><td>200</td><td>1423</td><td>{\"inputs\": [{\"query\": \"What are the results of CaP on different robot systems?\"}]}</td><td>{\"predictions\": [\"CaP has been tested on various robot systems and has shown competitive results compared to other baselines. In the table-top manipulation domain, CaP performed comparably to the CLIPort baseline on tasks with seen attributes and instructions, and outperformed it on unseen tasks and attributes. In the mobile robot navigation and manipulation domain, CaP was able to successfully execute unseen language commands, demonstrating its ability to follow long-horizon reactive commands with control structures and precise spatial reasoning. These results suggest that CaP can be effectively deployed across different robot systems with different APIs.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>b85ebac4-20f1-4004-815e-55e09bbbae00</td><td>2025-08-03</td><td>1754174982197</td><td>200</td><td>2201</td><td>{\"inputs\": [{\"query\": \"How have PFMs been applied in the field of Natural Language Processing (NLP)?\"}]}</td><td>{\"predictions\": [\"PFMs (Pretrained Foundation Models) have been widely applied in the field of Natural Language Processing (NLP) through the use of pretraining techniques. These models are trained on large benchmark datasets and then fine-tuned for specific tasks, allowing them to capture rich knowledge and improve performance in areas such as part-of-speech tagging, named entity recognition, semantic role labeling, machine translation, question answering, sentiment analysis, text summarization, text classification, relationship extraction, and event extraction. Examples of PFMs used in NLP include BERT and XLNet.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>bf319f87-d3fb-4438-b764-8ef8f074e448</td><td>2025-08-03</td><td>1754173644302</td><td>200</td><td>1842</td><td>{\"inputs\": [{\"query\": \"What is the role of reinforcement learning from human feedback (RLHF) in the context of PFMs, as exemplified by ChatGPT?\"}]}</td><td>{\"predictions\": [\"RLHF is a technique used to align language models (LMs) with human preferences and values, which can be challenging to capture with hard-coded reward functions. It involves using a pre-trained LM to generate text, which is then evaluated by humans. This data is used to learn a reward model that predicts a scalar reward for any generated text, which captures human preferences when judging model output. Finally, the LM is optimized against this reward model using RL policy gradient algorithms. In the context of PFMs like ChatGPT, RLHF can be applied after an initial supervised fine-tuning phase using a small number of expert demonstrations for the corresponding downstream task. This helps to ensure that the model's generations are of high quality and closely aligned with human intentions.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr><tr><td>null</td><td>0ae40a14-09b5-46be-a92d-5e837a352682</td><td>2025-08-03</td><td>1754173766514</td><td>200</td><td>1951</td><td>{\"inputs\": [{\"query\": \"How have PFMs been applied to multimodal data, and what are some examples of unified PFMs?\"}]}</td><td>{\"predictions\": [\"PFMs, or Pretrained Foundation Models, have been applied to multimodal data, which refers to data that consists of multiple types of information, such as text, images, and audio. Unified PFMs are models that can handle different types of data. Some examples of unified PFMs include GPT-4 by OpenAI, which can process both text and image inputs and generate text outputs, and OFA (Once for All), UNIFIED-IO, FLAVA, BEiT-3, and others. These models have demonstrated human-level performance on various professional and academic evaluation tasks.\"]}</td><td>1.0</td><td>Map(model_name -> amine_elhelou_ud8n_da.default.rag_app, endpoint_name -> amine_elhelou_ud8n_da_gaisd_endpoint, model_version -> 1)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         null,
         "5dca1319-2a0d-4d82-bbac-a10bde2c2e84",
         "2025-08-03",
         1754174434710,
         200,
         3066,
         "{\"inputs\": [{\"query\": \"Can you discuss the main challenges and opportunities for future research of very large models in the big data era, as mentioned in the text?\"}]}",
         "{\"predictions\": [\"Sure, I'd be happy to discuss the main challenges and opportunities for future research of very large models in the big data era.\\n\\nChallenges:\\n\\n1. Limited context size: Even though the size of language models has been increasing, most models still use a relatively small context size compared to the potentially large context needed to correctly perform language modeling. This can lead to issues when the necessary knowledge is not present in the context.\\n2. Storage of knowledge: Massive scale is required to store knowledge that is not present in the context but necessary to perform the task at hand. This can be a significant challenge in terms of both storage and computation.\\n3. Non-parametric models: By equipping language models with modules that retrieve information from external databases, it is possible to match certain capabilities of some of the largest language models while having fewer parameters. However, this results in non-parametric models, which can introduce new challenges in terms of data management and retrieval.\\n\\nOpportunities:\\n\\n1. Improved context relevance: By increasing the relevance of the context provided to language models, it is possible to improve their performance on certain tasks. This can be achieved through strategies such as adding information extracted from relevant external documents or allowing language models to leverage external tools.\\n2. Reasoning strategies: Language models can improve their context via reasoning strategies, which can lead to more relevant contexts and improved performance on certain tasks.\\n3. External tools: Allowing language models to leverage external tools can augment the current context with important missing information that\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "b66fd6af-c9a4-4672-ba39-9853a8ade8d4",
         "2025-08-03",
         1754176878826,
         200,
         3341,
         "{\"inputs\": [{\"query\": \"How does the study's exposure measure reflect the technical capacity to make human labor more efficient, and what factors imply that technical feasibility does not guarantee labor productivity or automation outcomes?\"}]}",
         "{\"predictions\": [\"The study's exposure measure reflects the technical capacity to make human labor more efficient by quantifying the exposure of different occupations to large language models (LLMs) like GPT-4. This measure is based on the assumption that as LLMs improve, they will be able to perform an increasing number of tasks currently done by humans, thereby making human labor more efficient.\\n\\nHowever, there are several factors that imply technical feasibility does not guarantee labor productivity or automation outcomes. First, the study finds that the AI Occupational Exposure Score is not correlated with the study's exposure measures, suggesting that the potential impact of LLMs on labor productivity and automation may not be fully captured by existing measures of AI exposure.\\n\\nSecond, the study finds that manual work is not exposed to LLMs or even LLMs with additional systems integration for the time being. This suggests that the technical feasibility of automating manual tasks with LLMs may not be sufficient to guarantee labor productivity or automation outcomes, as there may be other factors such as the cost of implementation or the availability of complementary technologies that also need to be considered.\\n\\nFinally, the study notes that the potential impact of LLMs expands significantly when complementary technologies are taken into account. This suggests that the technical feasibility of automating tasks with LLMs may depend on the development of other technologies, and that the relationship between technical feasibility and labor productivity or automation outcomes may be complex and multifaceted.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "b4bb72de-bd46-4f73-91f1-4d5774b56c08",
         "2025-08-03",
         1754176880893,
         200,
         1968,
         "{\"inputs\": [{\"query\": \"How do human assessments compare to GPT-4 classifications in the study's primary exposure dataset, and what is the main finding regarding the potential impact of LLMs like GPT-4 when considering both current model capabilities and anticipated tools built upon them?\"}]}",
         "{\"predictions\": [\"Human assessments and GPT-4 classifications show a high degree of agreement in the primary exposure dataset. The main finding regarding the potential impact of LLMs like GPT-4 suggests that while current model capabilities are significant, the impact on occupations may be even greater when considering anticipated tools built upon them. This is because LLMs have the potential to automate a wide range of tasks, including those that are currently considered non-automatable. However, the study also notes some limitations, such as the validity of the task-based framework and the lack of expertise and task interpretation by human annotators.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "199e33d2-6ead-4a70-88af-2c0c29cf5a7e",
         "2025-08-03",
         1754176884462,
         200,
         3408,
         "{\"inputs\": [{\"query\": \"What are the key findings regarding the relationship between exposure to LLMs and various factors such as wages, skillsets, and job preparation difficulties, and how do these findings compare to previous efforts documenting the distribution of automation exposure in the economy?\"}]}",
         "{\"predictions\": [\"1. Relationship between LLM exposure and wages: The study finds that occupational exposure to LLMs weakly increases with the difficulty of job preparation, but the relationship between LLM exposure and wages is not explicitly mentioned. It suggests that workers facing higher barriers to entry in their jobs tend to experience more exposure to LLMs.\\n\\n2. Skillsets and LLM exposure: The study does not provide a direct comparison between skillsets and LLM exposure. However, it mentions that LLMs like GPT-4 can assess tasks for exposure and automation potential, which could be used to evaluate skillsets in the future.\\n\\n3. Comparison to previous automation exposure efforts: The study's measurements are broadly consistent with previous efforts documenting the distribution of automation exposure in the economy. Most other technology exposure measures examined in the study are statistically significantly correlated with the preferred exposure measure, while measures of manual routineness and robotics exposure show negative correlations.\\n\\n4. Industry-specific LLM exposure: Information processing industries exhibit high exposure to LLMs, while manufacturing, agriculture, and mining demonstrate lower exposure.\\n\\n5. Productivity growth and LLM exposure: The connection between productivity growth in the past decade and overall LLM exposure appears weak, suggesting a potential optimistic case that future productivity gains from LLMs may not exacerbate possible cost disease effects.\\n\\n6. Pervasive impacts of LLMs: The study indicates that the impacts of LLMs are likely to be pervasive, with the potential impact expanding significantly when\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "ee3ba5f7-376d-436d-a816-4077a63690c8",
         "2025-08-03",
         1754175288565,
         200,
         3723,
         "{\"inputs\": [{\"query\": \"Can you discuss the history and evolution of PFMs in the NLP, CV, and Graph Learning (GL) domains?\"}]}",
         "{\"predictions\": [\"Sure, I'd be happy to discuss the history and evolution of Pretrained Foundation Models (PFMs) in the Natural Language Processing (NLP), Computer Vision (CV), and Graph Learning (GL) domains.\\n\\nIn the NLP domain, PFMs have evolved significantly over the years. Traditional models focused on static feature learning, but the introduction of the Transformer model revolutionized the field. The Transformer, which relies solely on attention mechanisms and doesn't use recurrent or convolutional architectures, allowed for the transfer of weighted representation knowledge between various neural units. This led to the development of models like GPT-3, a generative model based on the Transformer.\\n\\nIn the CV domain, the Vision Transformer (ViT) was proposed to represent an image as a series of image patches, similar to a series of word embeddings. This was a significant evolution from traditional CV models that relied on convolutional neural networks.\\n\\nIn the GL domain, Graph Transformer Networks (GTN) were employed to learn new graph structures and powerful node representations without domain knowledge. This was a significant advancement from traditional GL models that relied on graph convolutional networks.\\n\\nIn summary, the evolution of PFMs in these three domains has been marked by a shift from static feature learning to dynamic feature learning, and the Transformer model has played a crucial role in this evolution. The Transformer's ability to facilitate the transfer of weighted representation knowledge between various neural units has made it a key component in PFMs in NLP, CV, and GL.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "d0aa9544-4183-45ae-954e-4cff1b5c05e8",
         "2025-08-03",
         1754174131304,
         200,
         3996,
         "{\"inputs\": [{\"query\": \"What are the major advantages of PFMs, and how have they inspired research in model efficiency, security, and compression?\"}]}",
         "{\"predictions\": [\"1. Major Advantages of PFMs:\\nPFMs (Pretrained Foundation Models) have several major advantages:\\n   a. Versatility: PFMs can be adapted to a wide range of tasks and domains, including natural language processing (NLP), computer vision (CV), and graph learning (GL).\\n   b. Improved Performance: PFMs have shown significant improvements in performance across various tasks compared to models trained from scratch.\\n   c. Transfer Learning: PFMs enable transfer learning, where a model pretrained on a large dataset can be fine-tuned for a specific task with less data and computation.\\n\\n2. Inspiration in Model Efficiency, Security, and Compression:\\nThe success of PFMs has inspired research in model efficiency, security, and compression:\\n   a. Model Efficiency: Researchers are exploring ways to make PFMs more efficient, such as developing more efficient model architectures, optimizing training algorithms, and reducing the computational requirements for fine-tuning.\\n   b. Security: PFMs have raised concerns about security and privacy, leading to research on secure and private model training, such as federated learning and differential privacy.\\n   c. Compression: To make PFMs more practical for real-world applications, researchers are working on model compression techniques, such as pruning, quantization, and knowledge distillation, to reduce the size and computational requirements of PFMs.\\n\\nIn summary, PFMs have shown significant potential in various domains, and their success has inspired research in model efficiency, security, and compression to make them more\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "fafeadc1-1fd3-4fe5-a2f7-b446486ec53a",
         "2025-08-03",
         1754173582302,
         200,
         3636,
         "{\"inputs\": [{\"query\": \"How does the pretraining technique used in PFMs differ between the NLP, CV, and GL domains?\"}]}",
         "{\"predictions\": [\"In the NLP domain, pretraining techniques used in PFMs involve training a general model on large amounts of unlabeled text data, which can then be fine-tuned for various downstream tasks. This approach allows well-trained language models to capture rich knowledge, such as long-term dependencies and hierarchical relationships, that can be beneficial for downstream tasks. Early pretraining techniques in NLP were static, but dynamic pretraining techniques, such as BERT and XLNet, have since been proposed to better adapt to different semantic environments.\\n\\nIn the CV domain, pretraining techniques involve training a model on a large dataset of images, such as ImageNet, and then fine-tuning it for specific tasks, such as image classification or object detection. Pretrained models in CV can capture features such as shapes, textures, and colors, which can be useful for various computer vision tasks.\\n\\nIn the GL domain, pretraining techniques involve training a model on a large graph dataset, such as citation networks or social networks, and then fine-tuning it for specific tasks, such as node classification, link prediction, or graph classification. Pretrained models in GL can capture structural and relational information in graphs, which can be useful for various graph-based tasks.\\n\\nOverall, while the specific pretraining techniques used in PFMs may differ between the NLP, CV, and GL domains, they all involve training a general model on large amounts of data and then fine-tuning it for specific tasks. The main difference lies in the type of\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "a8b2637e-93fe-429f-908a-adf2e7da50ce",
         "2025-08-03",
         1754176635195,
         200,
         2319,
         "{\"inputs\": [{\"query\": \"How does the proposed assessing LLM capabilities and their potential effects on jobs complement predictions of technology's impacts on work, and what specific criteria does it include for measuring exposure to LLMs?\"}]}",
         "{\"predictions\": [\"The proposed rubric for assessing LLM capabilities and their potential effects on jobs complements predictions of technology's impacts on work by providing a framework to understand the evolving landscape of language models and their associated technologies. It measures the overall exposure of tasks to LLMs, which serves as a proxy for potential economic impact without distinguishing between labor-augmenting or labor-displacing effects.\\n\\nThe specific criteria included in the rubric for measuring exposure to LLMs are:\\n\\n1. The extent to which the task can be performed using text-based inputs and outputs.\\n2. The level of creativity, critical thinking, and complex communication required for the task.\\n3. The degree to which the task can be broken down into smaller, repeatable steps.\\n4. The availability and quality of data required to train LLMs for the task.\\n5. The potential for integrating LLMs with complementary technologies to enhance their capabilities.\\n\\nThese criteria help determine the level of exposure a task has to LLMs, considering both the current capabilities of LLMs and anticipated LLM-powered software.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "c2339dd8-edfe-4013-a33c-a047a7a124f5",
         "2025-08-03",
         1754176326963,
         200,
         1488,
         "{\"inputs\": [{\"query\": \"How does CaP compare against baselines in simulated language-instructed manipulation tasks?\"}]}",
         "{\"predictions\": [\"In the study, CaP (Language-Model-Assisted Policy Learning) was compared against several baselines in simulated language-instructed manipulation tasks. The results demonstrated that CaP outperformed the baselines in terms of success rate and sample efficiency. CaP was able to interpret natural language instructions, process perception outputs, and parameterize low-dimensional inputs to control primitives, which contributed to its superior performance. However, CaP is still limited by the scope of available perception APIs and control primitives, and it may struggle with significantly longer or more complex commands.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "b1d0fc49-143f-4836-9cf0-a29272a04427",
         "2025-08-03",
         1754176332023,
         200,
         2901,
         "{\"inputs\": [{\"query\": \"What are the limitations of CaP in the context of the experiments?\"}]}",
         "{\"predictions\": [\"1. Limited Generalization to Unseen Tasks: The performance of CaP degrades when it encounters tasks with unseen attributes or instructions, as observed in the table-top manipulation domain. This is because end-to-end systems like CLIPort struggle to generalize to new tasks that require precise numerical spatial-geometric reasoning.\\n\\n2. Dependence on Perception APIs: CaP relies on object detection APIs for perception, which may limit its performance if the APIs are not accurate or if they fail to detect certain objects.\\n\\n3. Limited Ability to Perform Precise Numerical Computations: While CaP outperforms natural-language planners in tasks that require precise numerical spatial-geometric reasoning, it still has limitations in its ability to perform precise numerical computations compared to direct coding.\\n\\n4. Limited Ability to Parse Complex Language Commands: CaP may struggle to parse complex language commands that require multiple steps or that involve ambiguous or unclear instructions.\\n\\n5. Limited Ability to Maintain a Session: CaP relies on a session-based system to maintain context between language commands, but this system may have limitations in its ability to maintain context over long periods of time or in complex scenarios.\\n\\n6. Limited Ability to Generate New Functions: While CaP can generate new functions, this ability is limited by the complexity of the functions and the availability of APIs to perform the required actions.\\n\\n7. Limited Ability to Draw Entirely New Shapes: While CaP can draw entirely new shapes from language commands, this ability is limited by the complexity of the shapes\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "d16e7054-ebae-4e03-a215-82d0ea119468",
         "2025-08-03",
         1754177126888,
         200,
         2124,
         "{\"inputs\": [{\"query\": \"What is the significance of the study's findings regarding the potential impact of LLMs like GPT-4, and how do these findings relate to the concept of general-purpose technologies?\"}]}",
         "{\"predictions\": [\"The study's findings suggest that LLMs, such as GPT-4, have the potential to significantly impact a wide range of occupations within the U.S. economy, which is a key attribute of general-purpose technologies. General-purpose technologies are characterized by their pervasiveness, improvement over time, and the development of significant co-invention and spillovers. The study's results indicate that LLMs could meet these criteria, as they can affect various tasks and occupations, potentially leading to widespread economic impacts. This is consistent with the idea that machine learning, as a broad category, is likely a general-purpose technology, and LLMs are a subset of this category that meet the criteria for general-purpose technology status independently.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "69da39b3-4d0d-4a54-ad4d-6195a40444f4",
         "2025-08-03",
         1754175350239,
         200,
         1544,
         "{\"inputs\": [{\"query\": \"What is the role of supervised fine-tuning (SFT) in instruction-aligning methods for language models (LMs)?\"}]}",
         "{\"predictions\": [\"Supervised Fine-Tuning (SFT) is a technique used in instruction-aligning methods for language models (LMs) to help them follow human intents and generate meaningful outputs. In SFT, a pretrained LM is fine-tuned using high-quality annotated data in a supervised manner. This process involves creating a template with input-output pairs and an instruction, which guides the LM to generate the desired output for a given input. SFT can help improve the usefulness and harmlessness of LMs, making them more aligned with human intentions and less likely to produce harmful or inappropriate content.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "f4d7e853-3c16-466f-b9d9-7e3e6e40d8bd",
         "2025-08-03",
         1754175415649,
         200,
         4757,
         "{\"inputs\": [{\"query\": \"How does reinforcement learning (RL) help enhance various NLP tasks, and what are some potential risks associated with its use?\"}]}",
         "{\"predictions\": [\"1. Reinforcement Learning (RL) in NLP: RL has been applied to enhance various Natural Language Processing (NLP) tasks such as machine translation, summarization, dialogue generation, image captioning, question generation, text-games, and more. RL treats language generation tasks as sequential decision-making problems, which helps optimize non-differentiable objectives. This method aligns large language models (LMs) with human preferences, leading to more accurate and contextually relevant responses.\\n\\n2. Potential Risks of RL in NLP:\\n\\n   a. Overfitting: There is a risk of overfitting to metrics that use neural networks, leading to nonsensical samples that score well on the metrics. This can result in LMs generating irrelevant or inappropriate responses, which may negatively impact user experience and trust.\\n\\n   b. Misalignment with Human Preferences: While RL can align LMs with human preferences, there is a risk of misalignment if the reward model is not accurately trained or if the human preferences are not accurately represented. This can lead to LMs generating responses that do not align with user intent or expectations.\\n\\n   c. Ethical and Social Risks: Ethical and social risks of harm from LMs are significant concerns in Supervised Fine-Tuning (SFT). RL can potentially exacerbate these risks if not properly managed, as LMs may generate responses that are biased, offensive, or harmful to certain groups or individuals.\\n\\nIn summary, RL can significantly enhance NLP\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "2f75b3fc-edb8-4ad7-a525-881e87cd2cf5",
         "2025-08-03",
         1754174797439,
         200,
         1981,
         "{\"inputs\": [{\"query\": \"What are Pretrained Foundation Models (PFMs) and how are they used in Artificial Intelligence (AI)?\"}]}",
         "{\"predictions\": [\"Pretrained Foundation Models (PFMs) are AI models that are first trained on large-scale data and can be fine-tuned for various downstream tasks across different data modalities. They are called \\\"foundation\\\" models because they provide a strong base for a wide range of applications. PFMs have been used in natural language processing (NLP), computer vision (CV), and graph learning (GL) tasks, demonstrating great potential in learning feature representations. They are trained using pretraining techniques, which involve training a general model on large amounts of data and tasks, and then fine-tuning it for specific applications. PFMs have shown superior performance in tasks such as text classification, text generation, image classification, object detection, and graph classification. Examples of PFMs include BERT, ChatGPT, and GPT-4.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "0171a696-60e8-414b-9448-ce0715643388",
         "2025-08-03",
         1754172631811,
         400,
         null,
         "{\"inputs\": [{\"query\": \"What is PPO?\"}]}",
         "{\"error_code\": \"BAD_REQUEST\", \"message\": \"1 tasks failed. Errors: {0: 'error: Exception(\\\\'Response content b\\\\\\\\\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\\\\\\\\\', status_code 403\\\\') Traceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 124, in issue_request\\\\n    response.raise_for_status()\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/requests/models.py\\\", line 1024, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://curriculum-dev.cloud.databricks.com/api/2.0/vector-search/endpoints/genai_vs_endpoint/indexes/amine_elhelou_ud8n_da.default.pdf_text_vs_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 326, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 299, in single_call_api\\\\n    response = self.lc_model(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\\\n    return self.invoke(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\\\n    self._call(inputs, run_manager=run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 141, in _call\\\\n    docs = self._get_docs(question, run_manager=_run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 221, in _get_docs\\\\n    return self.retriever.get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 323, in get_relevant_documents\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 316, in get_relevant_documents\\\\n    result = self._get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/vectorstores.py\\\", line 696, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 292, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 318, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/index.py\\\", line 223, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 127, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\', status_code 403\\\\n\\\\n request payload: {\\\\'query\\\\': \\\\'What is PPO?\\\\'}'}\"}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "173cc7ee-af54-4519-a299-e297537c29bb",
         "2025-08-03",
         1754172666336,
         200,
         2000,
         "{\"inputs\": [{\"query\": \"What is PPO?\"}]}",
         "{\"predictions\": [\"PPO, or Proximal Policy Optimization, is a type of reinforcement learning algorithm that aims to address the challenges of policy gradient methods by using a trust region to improve the stability and reliability of the learning process. It is designed to optimize a policy while ensuring that the new policy does not deviate too much from the old one, which helps to prevent drastic changes in the policy that could lead to instability or divergence. In the context of generative AI, PPO can be used to train models to generate realistic and diverse samples by optimizing a reward function that measures the quality of the generated samples.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "cf717436-eb9e-4410-8d05-9f0d9abce40a",
         "2025-08-03",
         1754172601224,
         400,
         null,
         "{\"inputs\": [{\"query\": \"What is PPO?\"}]}",
         "{\"error_code\": \"BAD_REQUEST\", \"message\": \"1 tasks failed. Errors: {0: 'error: Exception(\\\\'Response content b\\\\\\\\\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\\\\\\\\\', status_code 403\\\\') Traceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 124, in issue_request\\\\n    response.raise_for_status()\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/requests/models.py\\\", line 1024, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://curriculum-dev.cloud.databricks.com/api/2.0/vector-search/endpoints/genai_vs_endpoint/indexes/amine_elhelou_ud8n_da.default.pdf_text_vs_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 326, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 299, in single_call_api\\\\n    response = self.lc_model(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\\\n    return self.invoke(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\\\n    self._call(inputs, run_manager=run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 141, in _call\\\\n    docs = self._get_docs(question, run_manager=_run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 221, in _get_docs\\\\n    return self.retriever.get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 323, in get_relevant_documents\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 316, in get_relevant_documents\\\\n    result = self._get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/vectorstores.py\\\", line 696, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 292, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 318, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/index.py\\\", line 223, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 127, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\', status_code 403\\\\n\\\\n request payload: {\\\\'query\\\\': \\\\'What is PPO?\\\\'}'}\"}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "b39f15f9-bd44-4792-8f16-9a110308baec",
         "2025-08-03",
         1754172824814,
         200,
         1305,
         "{\"inputs\": [{\"query\": \"What is PPO?\"}]}",
         "{\"predictions\": [\"PPO, or Proximal Policy Optimization, is a type of reinforcement learning algorithm that aims to address the challenges of policy gradient methods. It does this by optimizing a surrogate objective function, which is designed to avoid large policy updates that could negatively impact performance. In the context of Generative AI, PPO can be used to train models to generate human-like text by optimizing the likelihood of generating human demonstrations.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "450c0629-7dae-4db3-b584-f3f1002dc971",
         "2025-08-03",
         1754174979763,
         200,
         2079,
         "{\"inputs\": [{\"query\": \"Can you explain the concept of pretraining in the context of PFMs and its origins in transfer learning?\"}]}",
         "{\"predictions\": [\"Pretraining in the context of PFMs (Pretrained Foundation Models) refers to the process of training a general model on large amounts of data and tasks, which can then be fine-tuned for specific downstream applications. The idea of pretraining originated from transfer learning in the field of computer vision (CV). It was later applied to the natural language processing (NLP) domain, where well-trained language models (LMs) can capture rich knowledge beneficial for downstream tasks. Pretraining techniques in NLP have the advantage of using unlimited amounts of unlabeled text corpus for training. The concept of pretraining has evolved over time, with early static techniques being replaced by dynamic methods like BERT and XLNet. Pretrained Foundation Models have demonstrated great potential in learning feature representations in various learning tasks, such as text classification, text generation, image classification, object detection, and graph classification.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "e26cc26f-e2d1-43c1-a9c2-0d0c885438aa",
         "2025-08-03",
         1754175224681,
         200,
         2202,
         "{\"inputs\": [{\"query\": \"What are some examples of PFMs in the field of Computer Vision (CV)?\"}]}",
         "{\"predictions\": [\"Some examples of Pretrained Foundation Models (PFMs) in the field of Computer Vision (CV) include:\\n\\n1. ViT-22B: This is a transformer-based model with about 22 billion parameters, which has achieved groundbreaking capabilities in various CV tasks.\\n2. Convolutional Neural Networks (CNNs): CNNs are a type of PFMs that have been widely used in CV for tasks such as image classification, object detection, and semantic segmentation. Examples of CNN-based PFMs include AlexNet, VGG, GoogLeNet, ResNet, and DenseNet.\\n3. Visual Transformers: Visual transformers are a type of PFMs that have been developed based on the transformer architecture, which was originally designed for natural language processing tasks. Examples of visual transformers include ViT, DeiT, and Swin Transformer.\\n\\nThese PFMs have been pretrained on large-scale datasets and can be fine-tuned for various downstream tasks in CV, such as image classification, object detection, and semantic segmentation.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "473dd70f-a19d-4358-9a13-a0c051663766",
         "2025-08-03",
         1754175899866,
         200,
         2033,
         "{\"inputs\": [{\"query\": \"What is the concept of chain-of-thought (CoT) prompting, and how does it improve the reasoning ability of LLMs?\"}]}",
         "{\"predictions\": [\"Chain-of-thought (CoT) prompting is a technique for improving the reasoning abilities of large language models (LLMs). It involves providing the model with a few examples of a task, including the inputs, intermediate reasoning steps, and the final output. This prompt serves as a guide for the model to follow when generating responses to new tasks. The CoT approach has been shown to outperform standard prompting methods, particularly when used with large-scale models such as LaMDA, PaLM, or GPT3. By providing a clear sequence of reasoning steps, CoT prompting helps LLMs to better understand and solve complex problems, even without additional training or fine-tuning.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "93c56e84-1682-43d7-b03a-b4312b74afc6",
         "2025-08-03",
         1754175963224,
         200,
         3033,
         "{\"inputs\": [{\"query\": \"How can CoT prompting be used in a practical example, and what are its benefits in improving the performance of LLMs on various reasoning tasks?\"}]}",
         "{\"predictions\": [\"CoT prompting can be used in a practical example by providing a prompt to the model that outlines a multi-step problem. For instance, if you ask the model \\\"After selling 30 out of his 100 chickens and 10 out of his 20 pigs, how many animals does a farmer have left?\\\", the model would generate a sequence of intermediate reasoning steps, such as \\\"The farmer has 100-30=70 chickens remaining\\\" and \\\"The farmer has 20-10=10 pigs remaining,\\\" before generating the final answer, such as \\\"The farmer has 70+10=80 animals remaining.\\\"\\n\\nThe benefits of CoT prompting in improving the performance of LLMs on various reasoning tasks include:\\n\\n1. Enhancing the ability of language models to reason about complicated problems.\\n2. Demonstrating efficacy in improving the performance of LLMs on various reasoning tasks, such as arithmetic, symbolic reasoning, and common sense.\\n3. Being a promising technique that can enhance the ability of language models to reason about complicated problems.\\n4. Showing slightly more harmless compared to without CoT.\\n5. Being an emergent property of model scale, meaning it works better with larger and more powerful language models.\\n6. Being possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "71ce55ed-f385-4e2e-9167-571d4d8c1444",
         "2025-08-03",
         1754176025164,
         200,
         1007,
         "{\"inputs\": [{\"query\": \"How does hierarchical code generation perform on code-generation benchmarks such as RoboCodeGen and HumanEval?\"}]}",
         "{\"predictions\": [\"Hierarchical code generation performs well on code-generation benchmarks such as RoboCodeGen and HumanEval. On RoboCodeGen, hierarchical code generation outperforms flat code generation, with larger models performing better. On HumanEval, hierarchical code generation also improves performance, with higher pass rates compared to flat code generation.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "47e9aa39-d1a3-497b-a98d-3ff0ad59627e",
         "2025-08-03",
         1754175717547,
         200,
         1654,
         "{\"inputs\": [{\"query\": \"How does GPT-4 utilize RLHF fine-tuning to improve the reliability, creativity, and capability of the model?\"}]}",
         "{\"predictions\": [\"GPT-4 utilizes RLHF (Reinforcement Learning with Human Feedback) fine-tuning to improve the reliability, creativity, and capability of the model by first pretraining the model to predict the subsequent token in a document. Afterward, the model undergoes RLHF fine-tuning, which involves training the model using reinforcement learning, where the reward model or machine labeler calculates a reward based on the model's output. This reward is then used to update the model using the Proximal Policy Optimization (PPO) algorithm. This process helps the model to better handle more nuanced instructions, be more creative, and more reliable in its responses.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "663c4b7f-9304-4d68-bdf0-e3e0a8cc5796",
         "2025-08-03",
         1754172487675,
         400,
         null,
         "{\"inputs\": [{\"query\": \"What is PPO?\"}]}",
         "{\"error_code\": \"BAD_REQUEST\", \"message\": \"1 tasks failed. Errors: {0: 'error: Exception(\\\\'Response content b\\\\\\\\\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\\\\\\\\\', status_code 403\\\\') Traceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 124, in issue_request\\\\n    response.raise_for_status()\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/requests/models.py\\\", line 1024, in raise_for_status\\\\n    raise HTTPError(http_error_msg, response=self)\\\\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://curriculum-dev.cloud.databricks.com/api/2.0/vector-search/endpoints/genai_vs_endpoint/indexes/amine_elhelou_ud8n_da.default.pdf_text_vs_index/query\\\\n\\\\nDuring handling of the above exception, another exception occurred:\\\\n\\\\nTraceback (most recent call last):\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 326, in call_api\\\\n    response = self.single_call_api(callback_handlers)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/mlflow/langchain/api_request_parallel_processor.py\\\", line 299, in single_call_api\\\\n    response = self.lc_model(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 378, in __call__\\\\n    return self.invoke(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 163, in invoke\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/base.py\\\", line 153, in invoke\\\\n    self._call(inputs, run_manager=run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 141, in _call\\\\n    docs = self._get_docs(question, run_manager=_run_manager)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py\\\", line 221, in _get_docs\\\\n    return self.retriever.get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\\\", line 148, in warning_emitting_wrapper\\\\n    return wrapped(*args, **kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 323, in get_relevant_documents\\\\n    raise e\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/retrievers.py\\\", line 316, in get_relevant_documents\\\\n    result = self._get_relevant_documents(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_core/vectorstores.py\\\", line 696, in _get_relevant_documents\\\\n    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 292, in similarity_search\\\\n    docs_with_score = self.similarity_search_with_score(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/langchain_community/vectorstores/databricks_vector_search.py\\\", line 318, in similarity_search_with_score\\\\n    search_resp = self.index.similarity_search(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/index.py\\\", line 223, in similarity_search\\\\n    response = RequestUtils.issue_request(\\\\n  File \\\"/opt/conda/envs/mlflow-env/lib/python3.10/site-packages/databricks/vector_search/utils.py\\\", line 127, in issue_request\\\\n    raise Exception(\\\\nException: Response content b\\\\'{\\\"error_code\\\":\\\"PERMISSION_DENIED\\\",\\\"message\\\":\\\"Insufficient permissions for UC entity amine_elhelou_ud8n_da.default.pdf_text_vs_index\\\"}\\\\', status_code 403\\\\n\\\\n request payload: {\\\\'query\\\\': \\\\'What is PPO?\\\\'}'}\"}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "e89a6539-5304-4726-927d-465b1c6724e0",
         "2025-08-03",
         1754176328776,
         200,
         1423,
         "{\"inputs\": [{\"query\": \"What are the results of CaP on different robot systems?\"}]}",
         "{\"predictions\": [\"CaP has been tested on various robot systems and has shown competitive results compared to other baselines. In the table-top manipulation domain, CaP performed comparably to the CLIPort baseline on tasks with seen attributes and instructions, and outperformed it on unseen tasks and attributes. In the mobile robot navigation and manipulation domain, CaP was able to successfully execute unseen language commands, demonstrating its ability to follow long-horizon reactive commands with control structures and precise spatial reasoning. These results suggest that CaP can be effectively deployed across different robot systems with different APIs.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "b85ebac4-20f1-4004-815e-55e09bbbae00",
         "2025-08-03",
         1754174982197,
         200,
         2201,
         "{\"inputs\": [{\"query\": \"How have PFMs been applied in the field of Natural Language Processing (NLP)?\"}]}",
         "{\"predictions\": [\"PFMs (Pretrained Foundation Models) have been widely applied in the field of Natural Language Processing (NLP) through the use of pretraining techniques. These models are trained on large benchmark datasets and then fine-tuned for specific tasks, allowing them to capture rich knowledge and improve performance in areas such as part-of-speech tagging, named entity recognition, semantic role labeling, machine translation, question answering, sentiment analysis, text summarization, text classification, relationship extraction, and event extraction. Examples of PFMs used in NLP include BERT and XLNet.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "bf319f87-d3fb-4438-b764-8ef8f074e448",
         "2025-08-03",
         1754173644302,
         200,
         1842,
         "{\"inputs\": [{\"query\": \"What is the role of reinforcement learning from human feedback (RLHF) in the context of PFMs, as exemplified by ChatGPT?\"}]}",
         "{\"predictions\": [\"RLHF is a technique used to align language models (LMs) with human preferences and values, which can be challenging to capture with hard-coded reward functions. It involves using a pre-trained LM to generate text, which is then evaluated by humans. This data is used to learn a reward model that predicts a scalar reward for any generated text, which captures human preferences when judging model output. Finally, the LM is optimized against this reward model using RL policy gradient algorithms. In the context of PFMs like ChatGPT, RLHF can be applied after an initial supervised fine-tuning phase using a small number of expert demonstrations for the corresponding downstream task. This helps to ensure that the model's generations are of high quality and closely aligned with human intentions.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ],
        [
         null,
         "0ae40a14-09b5-46be-a92d-5e837a352682",
         "2025-08-03",
         1754173766514,
         200,
         1951,
         "{\"inputs\": [{\"query\": \"How have PFMs been applied to multimodal data, and what are some examples of unified PFMs?\"}]}",
         "{\"predictions\": [\"PFMs, or Pretrained Foundation Models, have been applied to multimodal data, which refers to data that consists of multiple types of information, such as text, images, and audio. Unified PFMs are models that can handle different types of data. Some examples of unified PFMs include GPT-4 by OpenAI, which can process both text and image inputs and generate text outputs, and OFA (Once for All), UNIFIED-IO, FLAVA, BEiT-3, and others. These models have demonstrated human-level performance on various professional and academic evaluation tasks.\"]}",
         1.0,
         {
          "endpoint_name": "amine_elhelou_ud8n_da_gaisd_endpoint",
          "model_name": "amine_elhelou_ud8n_da.default.rag_app",
          "model_version": "1"
         }
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "client_request_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "databricks_request_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "timestamp_ms",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "status_code",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "execution_time_ms",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "request",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "response",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sampling_fraction",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "request_metadata",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inference_table_name = f\"{DA.catalog_name}.{DA.schema_name}.rag_app_realtime_payload\"\n",
    "display(spark.sql(f\"SELECT * FROM {inference_table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db9dca34-216f-4ff1-af6d-21b6a0486e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Define Evaluation Metrics\n",
    "In this task, you will define evaluation metrics such as toxicity, perplexity, and readability, which will be used to analyze the inference table data.\n",
    "\n",
    "-  Define the evaluation metrics functions using `pandas_udf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58137a2e-6b4c-4883-92dc-fd7d5b61d60e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-13 04:32:46.231730: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-08-13 04:32:46.236035: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2025-08-13 04:32:46.289272: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-08-13 04:32:47.503816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "\n",
    "## Import necessary libraries\n",
    "import tiktoken, textstat, evaluate\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "## Define a pandas UDF to compute the number of tokens in the text\n",
    "@pandas_udf(\"int\")\n",
    "def compute_num_tokens(texts: pd.Series) -> pd.Series:\n",
    "  encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "  return pd.Series(map(len, encoding.encode_batch(texts)))\n",
    "\n",
    "## Define a pandas UDF to compute the toxicity of the text\n",
    "@pandas_udf(\"double\")\n",
    "def compute_toxicity(texts: pd.Series) -> pd.Series:\n",
    "  ## Omit entries with null input from evaluation\n",
    "  toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\", cache_dir=\"/tmp/hf_cache/\")\n",
    "  return pd.Series(toxicity.compute(predictions=texts.fillna(\"\"))[\"toxicity\"]).where(texts.notna(), None)\n",
    "\n",
    "## Define a pandas UDF to compute the perplexity of the text\n",
    "@pandas_udf(\"double\")\n",
    "def compute_perplexity(texts: pd.Series) -> pd.Series:\n",
    "  ## Omit entries with null input from evaluation\n",
    "  perplexity = evaluate.load(\"perplexity\", module_type=\"measurement\", cache_dir=\"/tmp/hf_cache/\")\n",
    "  return pd.Series(perplexity.compute(data=texts.fillna(\"\"), model_id=\"gpt2\")[\"perplexities\"]).where(texts.notna(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f507c3b-ca27-44cd-813d-5ea951501d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Unpack the Request Payload\n",
    "In this task, you will unpack the request payload from the inference table and prepare it for processing.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- Unpack the requests as a stream.\n",
    "- Drop unnecessary columns for monitoring jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b27475df-6859-4e78-868c-e6948c97281f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## \n",
    "import os\n",
    "\n",
    "## Reset checkpoint [for demo purposes ONLY]\n",
    "checkpoint_location = os.path.join(DA.paths.working_dir, \"checkpoint\")\n",
    "dbutils.fs.rm(checkpoint_location, True)\n",
    "\n",
    "## Define the JSON path and type for the input requests\n",
    "INPUT_REQUEST_JSON_PATH = \"inputs[*].query\"\n",
    "INPUT_JSON_PATH_TYPE = \"array<string>\"\n",
    "KEEP_LAST_QUESTION_ONLY = False\n",
    "\n",
    "## Define the JSON path and type for the output responses\n",
    "OUTPUT_REQUEST_JSON_PATH = \"predictions\"\n",
    "OUPUT_JSON_PATH_TYPE = \"array<string>\"\n",
    "\n",
    "## Unpack the requests as a stream.\n",
    "requests_raw_df = spark.readStream.table(inference_table_name)\n",
    "requests_processed_df = unpack_requests(\n",
    "    requests_raw_df,\n",
    "    INPUT_REQUEST_JSON_PATH,\n",
    "    INPUT_JSON_PATH_TYPE,\n",
    "    OUTPUT_REQUEST_JSON_PATH,\n",
    "    OUPUT_JSON_PATH_TYPE,\n",
    "    KEEP_LAST_QUESTION_ONLY\n",
    ")\n",
    "\n",
    "## Drop un-necessary columns for monitoring jobs\n",
    "requests_processed_df = requests_processed_df.drop(\"date\", \"status_code\", \"sampling_fraction\", \"client_request_id\", \"databricks_request_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "528bb32b-601f-4691-90ce-4c1f01a84132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Compute Metrics\n",
    "\n",
    "In this task, you will compute the defined evaluation metrics for the unpacked request payloads.\n",
    "\n",
    "- Compute the toxicity, perplexity, and token count for the input and output columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "350a8225-646b-4906-80f5-4edeaa06f832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Define the columns to measure\n",
    "column_to_measure = [\"input\", \"output\"]\n",
    "\n",
    "## Iterate over each column to measure\n",
    "for column_name in column_to_measure:\n",
    "    ## Compute the metrics and add them as new columns to the DataFrame\n",
    "    requests_df_with_metrics = (\n",
    "      requests_processed_df\n",
    "                 .withColumn(f\"toxicity({column_name})\", compute_toxicity(col(column_name))) \n",
    "                 .withColumn(f\"perplexity({column_name})\", compute_perplexity(col(column_name))) \n",
    "                 .withColumn(f\"token_count({column_name})\", compute_num_tokens(col(column_name))) \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2488fec5-e279-43fa-b6de-8ced0669176d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Save the Processed Inference Table\n",
    "\n",
    "In this task, you will save the processed inference table with the computed metrics to a Delta table.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- Create the processed inference table if it doesn't exist.\n",
    "- Append the new unpacked payloads and metrics to the processed table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd14f9e-954a-4ded-b05a-45bf09837fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "## Define the name of the processed table\n",
    "processed_table_name = f\"{DA.catalog_name}.{DA.schema_name}.rag_app_processed_inferences_lab\"\n",
    "\n",
    "## Create the table if it does not exist\n",
    "(DeltaTable.createOrReplace(spark)\n",
    "        .tableName(processed_table_name)\n",
    "        .addColumns(requests_df_with_metrics.schema)\n",
    "        .property(\"delta.enableChangeDataFeed\", \"true\")\n",
    "        .property(\"delta.columnMapping.mode\", \"name\")\n",
    "        .execute())\n",
    "## Write the requests_df_with_metrics DataFrame to the processed table as a stream\n",
    "(requests_df_with_metrics.writeStream\n",
    "                      .trigger(availableNow=True)\n",
    "                      .format(\"delta\")\n",
    "                      .outputMode(\"append\")\n",
    "                      .option(\"checkpointLocation\", checkpoint_location)\n",
    "                      .toTable(processed_table_name).awaitTermination())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa7116a8-3806-4eae-963d-9e5f2f94b0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 5: Create a Monitor on the Inference Table\n",
    "\n",
    "In this task, you will create a monitor on the processed inference table using Databricks Lakehouse Monitoring.\n",
    "\n",
    "- Create a monitor using the `databricks-sdk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df2f9b8a-377d-4383-a8eb-97f549c6370c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import MonitorTimeSeries\n",
    "## Initialize the workspace client\n",
    "w = WorkspaceClient()\n",
    "\n",
    "try:\n",
    "  ## Create a monitor using the workspace client's quality_monitors service\n",
    "  lhm_monitor = w.quality_monitors.create(\n",
    "    table_name=processed_table_name,\n",
    "    time_series = MonitorTimeSeries(\n",
    "      timestamp_col = \"timestamp\",\n",
    "      granularities = [\"5 minutes\"],\n",
    "    ),\n",
    "    assets_dir = os.getcwd(),\n",
    "    slicing_exprs = [\"model_id\"],\n",
    "    output_schema_name=f\"{DA.catalog_name}.{DA.schema_name}\"\n",
    "  )\n",
    "\n",
    "## Handle any exceptions that occur during monitor creation\n",
    "except Exception as lhm_exception:\n",
    "  print(lhm_exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c75f03bc-3a0b-47e2-8606-cb0d245ece18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonitorInfoStatus.MONITOR_STATUS_PENDING\nWait until monitor creation is completed...\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "\n",
    "from databricks.sdk.service.catalog import MonitorInfoStatus\n",
    "\n",
    "## Get the monitor information for the processed table\n",
    "monitor_info = w.quality_monitors.get(processed_table_name)\n",
    "print(monitor_info.status)\n",
    "\n",
    "## Check if the monitor status is pending\n",
    "if monitor_info.status == MonitorInfoStatus.MONITOR_STATUS_PENDING:\n",
    "    print(\"Wait until monitor creation is completed...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4e3f547-dee5-4e12-b332-6520b6293a7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 6: Review the Monitor Details\n",
    "\n",
    "In this task, you will review the details of the monitor created in the previous step. This will involve checking the **Quality** tab for the monitor details and reviewing the metrics tables generated by the monitor.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "\n",
    "Complete following steps:\n",
    "\n",
    "\n",
    "1. **Review Monitor Details in Quality Tab**\n",
    "   - Go to the **[Catalog](explore/data)** and find the table you monitored.\n",
    "   - Click on the **Quality** tab to view the monitor details.\n",
    "\n",
    "2. **Review Metrics Tables**\n",
    "   - Examine the metrics tables (`*_processed_profile_metrics` and `*_processed_drift_metrics`).\n",
    "\n",
    "\n",
    "**\uD83D\uDEA8Note:** Ensure that the refresh process is completed and the metrics tables are ready before reviewing the details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e092e2c7-1c3e-4384-b183-3056b7ebe42e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 7: View the Monitor Dashboard\n",
    "\n",
    "In this task, you will view the Databricks SQL dashboard generated by Lakehouse Monitoring to review the data and metrics of your monitoring solution.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "Complete following steps:\n",
    "\n",
    "1. **View the SQL Dashboard**\n",
    "   - Click on **View Dashboard** to open the SQL dashboard from the **Quality** tab.\n",
    "\n",
    "2. **Inspect Overall Summary Statistics**\n",
    "   - Examine the overall summary statistics presented in the dashboard.\n",
    "\n",
    "3. **Review the Created Metrics**\n",
    "   - Review the metrics that were created in the first step of this lab to understand the data quality and model performance over time.\n",
    "\n",
    "\n",
    "**\uD83D\uDEA8Note:** Make sure there is an accessible DBSQL cluster up and running to ensure dashboard creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f687e63-aeb7-42b5-8b5e-c9e7e3ffe578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you created an online monitor using Databricks Lakehouse Monitoring. First, you defined evaluation metrics and computed these metrics for the inference table. Then, you created a monitor on the inference table. Lastly, you reviewed the monitor details and the auto-created Databricks SQL dashboard. After successfully completing this lab, you should be able to create online monitoring for an inference table that captures the inference requests of deployed AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3e0437-97b6-4abf-9fef-31d831f4ec6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"blank\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\" target=\"blank\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\" target=\"blank\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\" target=\"blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.LAB - Online Monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}